Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Pascanu2013,
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. Copyright 2013 by the author(s).},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
eprint = {1211.5063},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/84069287da0a6b488b8c933f3cb5be759cb6237e.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
number = {PART 3},
pages = {2347--2355},
title = {{On the difficulty of training recurrent neural networks}},
url = {https://arxiv.org/pdf/1211.5063.pdf},
year = {2013}
}
@article{Choi2019,
abstract = {Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where side information about the labels is used to unveil the relationship between each other. In this work, we investigate the zero-shot learning in the music domain and organize two different setups of side information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations. Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning. Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in the music domain.},
annote = {zero-shot approach,

project onto a semantic space of labels

able to predict unseen labels (newly added) by using side information (musical ionstrument annotation vectors, word embeddings)},
archivePrefix = {arXiv},
arxivId = {1907.02670},
author = {Choi, Jeong and Lee, Jongpil and Park, Jiyoung and Nam, Juhan},
eprint = {1907.02670},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/539632b9c468a6629e956f80915d31601816a93a.pdf:pdf},
title = {{Zero-shot Learning for Audio-based Music Classification and Tagging}},
url = {http://arxiv.org/abs/1907.02670},
year = {2019}
}
@article{Ramirez2019,
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
annote = {music genres are vague and unclear, very subjective (intro and 4)

Music genre classification (MGC) is a multi label classification problem --{\textgreater} problems of overlapping genres (2 ML)

GTZAN dataset beeing one of the most famouse one (4)

4.1 {\"{u}}ber featers die zur classification genutzt werden (MEL - erkl{\"{a}}rung und die n{\"{a}}he zum geh{\"{o}}r)

(5.1 GTZAN genaue erkl{\"{a}}rung mit hintergrund infos
--{\textgreater} Qualit{\"{a}}t hinterfragt)},
archivePrefix = {arXiv},
arxivId = {1911.12618},
author = {Ram{\'{i}}rez, Jaime and Flores, M. Julia},
doi = {10.1007/s10844-019-00582-9},
eprint = {1911.12618},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de25c85cb64dad1cfe855e9d60e99583bdb2c8f4.pdf:pdf},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification algorithms,Datasets,Feed-forward neural networks,Machine learning,Music,Music information retrieval},
title = {{Machine learning for music genre: multifaceted review and experimentation with audioset}},
url = {https://arxiv.org/pdf/1911.12618.pdf},
year = {2019}
}
@article{Tzanetakis2002,
abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a frame-work for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61{\%} for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
annote = {Classical approach (gaussian mixture model and k-nearest neighbor for a set of chosen extracted features)},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
file = {:C$\backslash$:/Users/SvenG/Downloads/download.pdf:pdf},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
month = {jul},
number = {5},
pages = {293--302},
title = {{Musical genre classification of audio signals}},
url = {http://ieeexplore.ieee.org/document/1021072/},
volume = {10},
year = {2002}
}
@article{Liu2019,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Recently, there has been increasing interest in attempting convolutional neural networks (CNNs) to achieve the task. However, most of existing methods employ the mature CNN structures proposed in image recognition without any modification, which results in the learning features that are not adequate for music genre classification. Faced with the challenge of this issue, we fully exploit the low-level information from spectrograms of audios and develop a novel CNN architecture in this paper. The proposed CNN architecture takes the long contextual information into considerations, which transfers more suitable information for the decision-making layer. Various experiments on several benchmark datasets, including GTZAN, Ballroom, and Extended Ballroom, have verified the excellent performances of the proposed neural network. Codes and model will be available at "ttps://github.com/CaifengLiu/music-genre-classification".},
annote = {komplizierter approach aber gute resultate
--{\textgreater} K{\"{o}}nnten wir versuchen


"Spectrograms of music data have been proved to be one effective tool to describe audio signals" (intro)

"it is neces- sary to design a specific CNN structure which can com- prehensively handle multi-scale of audio features."(intro)

"exploit an appropriate CNN model which can make full use of both the high-level semantic infor- mation and the low-level features from various music."(intro)

developed a "Bottom-up Broadcast Neural Network (BBNN), which adopts a relatively wide and shallow structure"


BM archi- tecture is that it maximally transmit and preserve all extracted feature maps to higher-layers so that the decision layers make a prediction based on all feature-maps in the network. Another practically useful aspect of BM design is that it aligns with the intuition that audio information should be perceived at various time-frequency scales simultaneously. (2.1)},
archivePrefix = {arXiv},
arxivId = {1901.08928},
author = {Liu, Caifeng and Feng, Lin and Liu, Guochao and Wang, Huibing and Liu, Shenglan},
eprint = {1901.08928},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ffcdc67c4ab342932bda320aa0217da88476e3de.pdf:pdf},
keywords = {all rights reserved,c 2019 elsevier ltd,cnn,music genre classification,spectrogram},
pages = {1--7},
title = {{Bottom-up Broadcast Neural Network For Music Genre Classification}},
url = {http://arxiv.org/abs/1901.08928},
year = {2019}
}
@article{Dokania2019,
abstract = {Music genre is arguably one of the most important and discriminative information for music and audio content. Visual representation based approaches have been explored on spectrograms for music genre classification. However, lack of quality data and augmentation techniques makes it difficult to employ deep learning techniques successfully. We discuss the application of graph neural networks on such task due to their strong inductive bias, and show that combination of CNN and GNN is able to achieve state-of-the-art results on GTZAN, and AudioSet (Imbalanced Music) datasets. We also discuss the role of Siamese Neural Networks as an analogous to GNN for learning edge similarity weights. Furthermore, we also perform visual analysis to understand the field-of-view of our model into the spectrogram based on genre labels.},
annote = {GNNs and siamese network

used GTZAN dataset
--{\textgreater} "failes to capture recent trends in music"

70:30 train:test split},
archivePrefix = {arXiv},
arxivId = {1910.11117},
author = {Dokania, Shubham and Singh, Vasudev},
eprint = {1910.11117},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/25701c5d7e75b800245a9e103abd9fe6f7dda123.pdf:pdf},
month = {oct},
number = {2017},
title = {{Graph Representation learning for Audio {\&} Music genre Classification}},
url = {http://arxiv.org/abs/1910.11117},
year = {2019}
}
@article{Feng2017,
abstract = {Deep learning has been demonstrated its effectiveness and efficiency in music genre classification. However, the existing achievements still have several shortcomings which impair the performance of this classification task. In this paper, we propose a hybrid architecture which consists of the paralleling CNN and Bi-RNN blocks. They focus on spatial features and temporal frame orders extraction respectively. Then the two outputs are fused into one powerful representation of musical signals and fed into softmax function for classification. The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.},
annote = {Cnn and RNN combinations (parrallel model)},
archivePrefix = {arXiv},
arxivId = {1712.08370},
author = {Feng, Lin and Liu, Shenlan and Yao, Jianing},
eprint = {1712.08370},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5f23aadfd868258a104d86eada9f6e856063135f.pdf:pdf},
number = {December},
pages = {1--13},
title = {{Music Genre Classification with Paralleling Recurrent Convolutional Neural Network}},
url = {http://arxiv.org/abs/1712.08370},
year = {2017}
}
@article{Karatana2017,
abstract = {Categorizing music files according to their genre is a challenging task in the area of music information retrieval (MIR). In this study, we compare the performance of two classes of models. The first is a deep learning approach wherein a CNN model is trained end-to-end, to predict the genre label of an audio signal, solely using its spectrogram. The second approach utilizes hand-crafted features, both from the time domain and the frequency domain. We train four traditional machine learning classifiers with these features and compare their performance. The features that contribute the most towards this multi-class classification task are identified. The experiments are conducted on the Audio set data set and we report an AUC value of 0.894 for an ensemble classifier which combines the two proposed approaches.},
annote = {spectogram analysis vs hand crafted features

compares multiple ml classsifiers},
archivePrefix = {arXiv},
arxivId = {1804.01149},
author = {Karatana, Ali and Yildiz, Oktay},
doi = {10.1109/siu.2017.7960694},
eprint = {1804.01149},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/7915f9109576eeb26e33dad27b705efd509a1d17.pdf:pdf},
pages = {1--4},
title = {{Music genre classification with machine learning techniques}},
url = {https://arxiv.org/pdf/1804.01149.pdf},
year = {2017}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.

https://github.com/ds7711/music{\_}genre{\_}classification/blob/master/lda{\_}analysis.ipynb

confusion matrix code

This paper uses a "divide-and-conquer" approach to solve the classificationt ask.
1. split spectogram into 3 - second segements
2. make predicitons for each segment, combine predictions.
3 second approach based on previous papers. Input is mel-spectogram, which approxiamtes humans auditory system workings},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Viswanathan2016,
abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
annote = {unser paper},
author = {Viswanathan, Ajay Prasadh},
doi = {10.18535/ijecs/v4i10.38},
file = {:C$\backslash$:/Users/SvenG/Downloads/21.pdf:pdf},
journal = {International Journal Of Engineering And Computer Science},
title = {{Music Genre Classification}},
year = {2016}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/75a1126f6710eeb85af855eb2b0d80946fcc6b6e.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1411.4038.pdf http://arxiv.org/abs/1411.4038},
volume = {39},
year = {2014}
}
@book{Szeliski2011,
author = {Szeliski, Richard},
booktitle = {Phylogenetic Networks},
doi = {10.1017/cbo9780511974076.010},
file = {:C$\backslash$:/Users/SvenG/Downloads/(Texts in Computer Science) Richard Szeliski - Computer Vision{\_} Algorithms and Applications-Springer (2011).pdf:pdf},
isbn = {9781848829343},
pages = {185--186},
publisher = {Springer},
title = {{Computer Vision: Algorithms and Applications}},
year = {2011}
}
@article{Gastal2010,
abstract = {Image matting aims at extracting foreground elements from an image by means of color and opacity (alpha) estimation. While a lot of progress has been made in recent years on improving the accuracy of matting techniques, one common problem persisted: the low speed of matte computation. We present the first real-time matting technique for natural images and videos. Our technique is based on the observation that, for small neighborhoods, pixels tend to share similar attributes. Therefore, independently treating each pixel in the unknown regions of a trimap results in a lot of redundant work. We show how this computation can be significantly and safely reduced by means of a careful selection of pairs of background and foreground samples. Our technique achieves speedups of up to two orders of magnitude compared to previous ones, while producing high-quality alpha mattes. The quality of our results has been verified through an independent benchmark. The speed of our technique enables, for the first time, real-time alpha matting of videos, and has the potential to enable a new class of exciting applications. {\textcopyright} 2010 The Eurographics Association and Blackwell Publishing Ltd.},
author = {Gastal, Eduardo S.L. and Oliveira, Manuel M.},
doi = {10.1111/j.1467-8659.2009.01627.x},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/074e39a1c533993dcc829d9996c6518608d01e49.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {I.4.6 [Image Processing and Computer Vision]: Segm},
number = {2},
pages = {575--584},
title = {{Shared sampling for real-time alpha matting}},
url = {https://www.inf.ufrgs.br/{~}eslgastal/SharedMatting/Gastal{\_}Oliveira{\_}EG2010{\_}Shared{\_}Matting.pdf},
volume = {29},
year = {2010}
}
@misc{Christopher2015,
author = {Olah, Christopher},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5192834415fb75e40f97944b7d64f8980b4c1ec5.html:html},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2020-05-28},
year = {2015}
}
@article{Dhanachandra2015,
abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
doi = {10.1016/j.procs.2015.06.090},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/image-segmentation-using-k-means-clustering-algorithm-and-subtractive-clustering-algorithm (1).pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Image segmentation,K -means clustering,Median filter,Partial contrast stretching,Subtractive clustering},
pages = {764--771},
publisher = {Elsevier Masson SAS},
title = {{Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm}},
url = {http://dx.doi.org/10.1016/j.procs.2015.06.090 https://linkinghub.elsevier.com/retrieve/pii/S1877050915014143},
volume = {54},
year = {2015}
}
@article{Han2019,
abstract = {Recent studies have greatly promoted the development of semantic segmentation. Most state-of-the-art methods adopt fully convolutional networks (FCNs) to accomplish this task, in which the fully connected layer is replaced with the convolution layer for dense prediction. However, standard convolution has limited ability in maintaining continuity between predicted labels as well as forcing local smooth. In this paper, we propose the dense convolution unit (DCU), which is more suitable for pixel-wise classification. The DCU adopts dense prediction instead of the center-prediction manner used in current convolution layers. The semantic label for every pixel is inferred from those overlapped center/off-center predictions from the perspective of probability. It helps to aggregate contexts and embeds connections between predictions, thus successfully generating accurate segmentation maps. DCU serves as the classification layer and is a better option than standard convolution in FCNs. This technique is applicable and beneficial to FCN-based state-of-the-art methods and works well in generating segmentation results. Ablation experiments on benchmark datasets validate the effectiveness and generalization ability of the proposed approach in semantic segmentation tasks.},
author = {Han, Chaoyi and Duan, Yiping and Tao, Xiaoming and Lu, Jianhua},
doi = {10.1109/ACCESS.2019.2908685},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/94f424c9bbaa0abb71874ddac144eab505539f48.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Dense convolution unit,fully convolutional network,overlapped prediction,semantic segmentation},
pages = {43369--43382},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf},
volume = {7},
year = {2014}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao},
doi = {[]},
eprint = {1506.04214},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4a861d29f36d2e4f03477c5df2730c579d8394d3.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {2--3},
title = {{Convolutional LSTM Network}},
url = {http://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting},
year = {2015}
}
@incollection{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/20a78d3145279dcd799cd7a856ae2714f4863a16.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
keywords = {Convolutional neural network,Real-time object detection},
month = {dec},
pages = {21--37},
title = {{SSD: Single Shot MultiBox Detector}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0{\_}2},
volume = {9905 LNCS},
year = {2016}
}
@inproceedings{Pfeuffer2019,
abstract = {Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNNbased approach for up to 1.6 percent.},
annote = {vergleicht SegNet + LSTM mit ICNet + LSTM

-{\textgreater} verbesserte resultate, jedoch teilweise l{\"{a}}ngere comp. dauer},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {Pfeuffer, Andreas and Schulz, Karina and Dietmayer, Klaus},
doi = {10.1109/IVS.2019.8813852},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfeuffer, Schulz, Dietmayer - 2019 - Semantic segmentation of video sequences with convolutional LSTMs.pdf:pdf},
isbn = {9781728105604},
title = {{Semantic Segmentation of Video Sequences with Convolutional LSTMs}},
url = {https://arxiv.org/pdf/1905.01058.pdf},
year = {2019}
}
@article{AssessmentandTeachingof21stCenturySkills2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Assessment and Teaching of 21st Century Skills}},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SvenG/Downloads/azure.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Kinect Documentation}},
volume = {53},
year = {2013}
}
@article{Chen2018,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0690ba31424310a90028533218d0afd25a829c8d.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
month = {jun},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1412.7062.pdf http://ieeexplore.ieee.org/document/7913730/ http://arxiv.org/abs/1606.00915},
volume = {40},
year = {2016}
}
@article{Nabavi2018,
abstract = {We consider the problem of predicting semantic segmentation of future frames in a video. Given several observed frames in a video, our goal is to predict the semantic segmentation map of future frames that are not yet observed. A reliable solution to this problem is useful in many applications that require real-time decision making, such as autonomous driving. We propose a novel model that uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal information of observed frames for future prediction. We also extend our model to use bidirectional ConvLSTM to capture temporal information in both directions. Our proposed approach outperforms other state-of-the-art methods on the benchmark dataset.},
annote = {bidirectional LSTM},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {shahabeddin Nabavi, Seyed and Rochan, Mrigank and Yang and Wang},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/651e844eaa55e9b91b77f84d76dd997cd9141554.pdf:pdf},
journal = {British Machine Vision Conference 2018, BMVC 2018},
month = {jul},
pages = {1--12},
title = {{Future Semantic Segmentation with Convolutional LSTM}},
url = {https://arxiv.org/pdf/1807.07946.pdf http://arxiv.org/abs/1807.07946},
year = {2018}
}
@article{Harley2017,
abstract = {We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a 'segmentation-aware' variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can replace DenseCRF inference with a cascade of segmentation-aware filters, and in optical flow we obtain clearly sharper responses than the ones obtained with comparable networks that do not use segmentation. In both cases segmentation-aware convolution yields systematic improvements over strong baselines.},
annote = {Introduction:
"The low-resolution issue has received substantial attention: for instance methods have been proposed for replacing the subsampling layers with resolution-preserving alternatives such as atrous convolution [9, 58, 43], or restoring the lost resolution via upsampling stages [39, 34]"


local foreground-backgreound segmentation mask
--{\textgreater} to enhance sharpness

3 steps:
(i) learn segmentation cues, (ii) use the cues to create local foreground masks, and (iii) use the masks together with convolution, to create foreground-focused convolution

--{\textgreater} genauer lesen},
archivePrefix = {arXiv},
arxivId = {1708.04607},
author = {Harley, Adam W. and Derpanis, Konstantinos G. and Kokkinos, Iasonas},
doi = {10.1109/ICCV.2017.539},
eprint = {1708.04607},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f4b4bf63e1059645301d338ff792ba43a7361f98.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5048--5057},
title = {{Segmentation-Aware Convolutional Networks Using Local Attention Masks}},
url = {https://arxiv.org/pdf/1708.04607.pdf},
volume = {2017-Octob},
year = {2017}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/87ba68678a1ae983cee474e4bfdd27257e45ca3d.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
month = {apr},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
url = {https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf http://link.springer.com/10.1007/BF00344251},
volume = {36},
year = {1980}
}
@article{Smith1979,
author = {Nobuyuki, Otsu},
doi = {10.1109/TSMC.1979.4310076},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/A{\_}Threshold{\_}Selection{\_}Method{\_}from{\_}gray-level{\_}Histograms.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {1},
pages = {62--66},
title = {{A Threshold Selection Method from Gray-Level Histograms}},
volume = {9},
year = {1979}
}
@article{Minaee2020,
abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
archivePrefix = {arXiv},
arxivId = {2001.05566},
author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
eprint = {2001.05566},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/dc6b78923fdd6d8f767a8d1145fd2b6fc847e249.pdf:pdf},
pages = {1--23},
title = {{Image Segmentation Using Deep Learning: A Survey}},
url = {http://arxiv.org/abs/2001.05566},
year = {2020}
}
@article{Sherstinsky2020,
abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
annote = {math equation of rnn},
archivePrefix = {arXiv},
arxivId = {1808.03314},
author = {Sherstinsky, Alex},
doi = {10.1016/j.physd.2019.132306},
eprint = {1808.03314},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/d8556d24d62e5b242f8d544c7b58b5973a3696fd.pdf:pdf},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
keywords = {Convolutional input context windows,External input gate,LSTM,RNN,RNN unfolding/unrolling},
month = {mar},
number = {March},
pages = {132306},
title = {{Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network}},
url = {https://arxiv.org/pdf/1808.03314.pdf https://linkinghub.elsevier.com/retrieve/pii/S0167278919305974},
volume = {404},
year = {2020}
}
@misc{Microsoft2019,
abstract = {This page covers how to use the depth camera in your Azure Kinect DK. The depth camera is the second of the two cameras. As covered in previous sections, the other camera is the RGB camera},
author = {Sych, Tetyana and Brent, Allen and Phil, Meadows and Microsoft},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c8bf20a2e5d841760fcee5c581f634d7052f749c.html:html},
title = {depth-camera @ docs.microsoft.com},
url = {https://docs.microsoft.com/de-de/azure/Kinect-dk/depth-camera},
urldate = {2020-03-04},
year = {2019}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/56142d781c2c1231ffbda59efb6f96fc7b5b5b52.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {may},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1505.04366.pdf http://arxiv.org/abs/1505.04366},
volume = {2015 Inter},
year = {2015}
}
@article{Cvpr2020,
abstract = {To satisfy the stringent requirements on computational resources in the field of real-time semantic segmentation, most approaches focus on the hand-crafted design of light-weight segmentation networks. Recently, Neural Architecture Search (NAS) has been used to search for the optimal building blocks of networks automatically, but the network depth, downsampling strategy, and feature aggregation way are still set in advance by trial and error. In this paper, we propose a joint search framework, called AutoRTNet, to automate the design of these strategies. Specifically, we propose hyper-cells to jointly decide the network depth and downsampling strategy, and an aggregation cell to achieve automatic multi-scale feature aggregation. Experimental results show that AutoRTNet achieves 73.9{\%} mIoU on the Cityscapes test set and 110.0 FPS on an NVIDIA TitanXP GPU card with 768x1536 input images.},
archivePrefix = {arXiv},
arxivId = {2003.14226},
author = {Sun, Peng and Wu, Jiaxiang and Li, Songyuan and Lin, Peiwen and Huang, Junzhou and Li, Xi},
eprint = {2003.14226},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/b86e9f4df22ff5a339b1597a25bc1cb52062dd13.pdf:pdf},
keywords = {neural,real-time semantic segmentation},
month = {mar},
pages = {1--15},
title = {{Real-Time Semantic Segmentation via Auto Depth, Downsampling Joint Decision and Feature Aggregation}},
year = {2020}
}
@article{Philipp2017,
abstract = {Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities "solve" the exploding gradient problem, we show that this is not the case in general and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients. ResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks. We show this is a direct consequence of the Pythagorean equation. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.},
archivePrefix = {arXiv},
arxivId = {1712.05577},
author = {Philipp, George and Song, Dawn and Carbonell, Jaime G.},
eprint = {1712.05577},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/55f673c7056fcfe001b3eee3d26b8867230d4476.pdf:pdf},
keywords = {deep learning,exploding gradients,neural networks,residual networks,van-},
number = {2014},
title = {{The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions}},
url = {http://arxiv.org/abs/1712.05577},
year = {2017}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381.pdf:pdf},
month = {dec},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@misc{COCO2016,
author = {{COCO Consortium}},
title = {{COCO - Common Objects in Context}},
url = {http://cocodataset.org/{\#}home http://mscoco.org/dataset/{\#}detections-leaderboard},
urldate = {2020-03-06},
year = {2016}
}
@book{Skansi2018,
address = {Cham},
author = {Skansi, Sandro},
doi = {10.1007/978-3-319-73004-2},
editor = {Mackie, Ian},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Dokumente/ML books/2018{\_}Book{\_}IntroductionToDeepLearning.pdf:pdf},
isbn = {978-3-319-73003-5},
pages = {187},
publisher = {Springer International Publishing},
series = {Undergraduate Topics in Computer Science},
title = {{Introduction to Deep Learning}},
url = {http://link.springer.com/10.1007/978-3-319-73004-2},
year = {2018}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ac723ee6a5c524d4139ff2ef177dbf1f0889a983.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {https://arxiv.org/pdf/1311.2524.pdf},
year = {2014}
}
@book{Suzuki2017,
author = {Suzuki, Kenji},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/40bd52ca059a531a7c0eb9742518f9624b5d8b5c.pdf:pdf},
isbn = {9789533072203},
number = {January 2013},
pages = {265},
title = {{Artificial Neural Networks - Methodological Advances and Biomedical Applications}},
url = {https://www.researchgate.net/profile/Kenji{\_}Suzuki4/publication/319316102{\_}Artificial{\_}Neural{\_}Networks{\_}-{\_}Methodological{\_}Advances{\_}and{\_}Biomedical{\_}Applications/links/59a42f16aca272a6461bb35e/Artificial-Neural-Networks-Methodological-Advances-and-Biomedical-Appl},
year = {2017}
}
@article{Chen2018a,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/85c171cc3862c5d922632f94387db267ccbbe7cc.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
month = {jun},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1606.00915.pdf http://ieeexplore.ieee.org/document/7913730/ http://arxiv.org/abs/1606.00915},
volume = {40},
year = {2016}
}
@inproceedings{Valipour2017,
abstract = {Image segmentation is an important step in most visual tasks. While convolutional neural networks have shown to perform well on single image segmentation, to our knowledge, no study has been done on leveraging recurrent gated architectures for video segmentation. Accordingly, we propose and implement a novel method for online segmentation of video sequences that incorporates temporal data. The network is built from a fully convolutional network and a recurrent unit that works on a sliding window over the temporal data. We use convolutional gated recurrent unit that preserves the spatial information and reduces the parameters learned. Our method has the advantage that it can work in an online fashion instead of operating over the whole input batch of video frames. The network is tested on video segmentation benchmarks in Segtrack V2 and Davis. It proved to have 5{\%} improvement in Segtrack and 3{\%} improvement in Davis in F-measure over a plain fully convolutional network.},
annote = {LSTM and GRU outperform traditional recurrent architecutres (s2)

Nice RNN explanation
"Recurrent Neural Networks[25] are designed to incorporate sequential information into a neural network framework. " (s3)

"The gates control back propagation flow between each node" (s3)

- difference LSTM and GRU (s4)},
archivePrefix = {arXiv},
arxivId = {1606.00487},
author = {Valipour, Sepehr and Siam, Mennatullah and Jagersand, Martin and Ray, Nilanjan},
booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2017.11},
eprint = {1606.00487},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/3d04bc62a30f4d6329db2099cc318cb89974a872.pdf:pdf},
isbn = {978-1-5090-4822-9},
month = {mar},
pages = {29--36},
publisher = {IEEE},
title = {{Recurrent Fully Convolutional Networks for Video Segmentation}},
url = {https://arxiv.org/pdf/1606.00487.pdf http://ieeexplore.ieee.org/document/7926594/},
year = {2017}
}
@misc{Fei-Fei2017,
author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Johnson, Yeung - Unknown - Lecture 11 Detection and Segmentation.pdf:pdf},
pages = {17},
title = {{Lecture 11: Detection and Segmentation}},
url = {http://cs231n.stanford.edu/slides/2017/cs231n{\_}2017{\_}lecture11.pdf},
urldate = {2020-03-04},
year = {2017}
}
@article{Sharma2020,
abstract = {In this paper, we propose an end to end solution for image matting i.e high-precision extraction of foreground objects from natural images. Image matting and background detection can be achieved easily through chroma keying in a studio setting when the background is either pure green or blue. Nonetheless, image matting in natural scenes with complex and uneven depth backgrounds remains a tedious task that requires human intervention. To achieve complete automatic foreground extraction in natural scenes, we propose a method that assimilates semantic segmentation and deep image matting processes into a single network to generate detailed semantic mattes for image composition task. The contribution of our proposed method is two-fold, firstly it can be interpreted as a fully automated semantic image matting method and secondly as a refinement of existing semantic segmentation models. We propose a novel model architecture as a combination of segmentation and matting that unifies the function of upsampling and downsampling operators with the notion of attention. As shown in our work, attention guided downsampling and upsampling can extract high-quality boundary details, unlike other normal downsampling and upsampling techniques. For achieving the same, we utilized an attention guided encoder-decoder framework which does unsupervised learning for generating an attention map adaptively from the data to serve and direct the upsampling and downsampling operators. We also construct a fashion e-commerce focused dataset with high-quality alpha mattes to facilitate the training and evaluation for image matting.},
annote = {The major reason behind the fact that SegNet is better in recovering boundary details is that unpooling utilizes max- pooling's index guidance for upsampling. On the other hand, bi-linearly interpolated feature maps fail to emphasize on the boundary details. In order to record the boundary locations, the responses of the shallow layers of the network are used to project the excitation of various index locations on the feature maps into an attention mask},
archivePrefix = {arXiv},
arxivId = {2003.03613},
author = {Sharma, Rishab and Deora, Rahul and Vishvakarma, Anirudha},
eprint = {2003.03613},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/1c277b80530f47601aed44896db8adeda88f7fbe.pdf:pdf},
title = {{AlphaNet: An Attention Guided Deep Network for Automatic Image Matting}},
url = {http://arxiv.org/abs/2003.03613},
year = {2020}
}
@incollection{Chen2018b,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89{\%} and 82.1{\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
archivePrefix = {arXiv},
arxivId = {1802.02611},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_49},
eprint = {1802.02611},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/503c16d9cb1560f13a7d6baedf8c9f889b22459d.pdf:pdf},
isbn = {9783030012335},
issn = {16113349},
keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
month = {feb},
pages = {833--851},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {https://arxiv.org/pdf/1802.02611.pdf http://link.springer.com/10.1007/978-3-030-01234-2{\_}49},
volume = {11211 LNCS},
year = {2018}
}
@article{Hochreiter1991,
abstract = {Ich versichere, da{\ss} ich diese Diplomarbeit selbst{\"{a}}ndig verfa{\ss}t und keine anderen als die ange-geben Quellen und Hilfsmittel benutzt habe.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Hochreiter, Sepp},
eprint = {1511.07289},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f5f21cc907bc296e55ae87ce1319dfda644435b2.pdf:pdf},
issn = {18168957 18163459},
journal = {Master's thesis, Institut f{\"{u}}r Informatik, Technische Universit{\"{a}}t, Munchen},
pages = {1--71},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
url = {http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
year = {1991}
}
@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/fcd44ab4ebeeb504481350732f21701bf5269fb4.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
month = {nov},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://arxiv.org/pdf/1511.00561.pdf http://arxiv.org/abs/1511.00561},
volume = {39},
year = {2015}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4c17b93b0b486522da5a7c2a4de0e077253c20d2.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
month = {mar},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
volume = {42},
year = {2017}
}
@article{Rieder2019,
annote = {used for image on page 10},
author = {Rieder, Mathias and Verbeet, Richard},
doi = {10.15480/882.2466},
file = {:C$\backslash$:/Users/SvenG/Downloads/Rieder{\_}Verbeet-Robot-Human-Learning{\_}for{\_}Robotic{\_}Picking{\_}Processes{\_}hicl{\_}2019.pdf:pdf},
keywords = {Computer Vision,Machine Learning,Object Detection,Picking Robots},
number = {October},
title = {{Robot-Human-Learning for Robotic Picking Processes}},
year = {2019}
}
@article{Yurdakul2017,
abstract = {Semantic segmentation of videos using neural networks is currently a popular task, the work done in this field is however mostly on RGB videos. The main reason for this is the lack of large RGBD video datasets, annotated with ground truth information at the pixel level. In this work, we use a synthetic RGBD video dataset to investigate the contribution of depth and temporal information to the video segmentation task using convolutional and recurrent neural network architectures. Our experiments show the addition of depth information improves semantic segmentation results and exploiting temporal information results in higher quality output segmentations.},
annote = {combines depth + rgb + videos},
author = {Yurdakul, Ekrem Emre and Yemez, Yucel},
doi = {10.1109/ICCVW.2017.51},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c02dbf756b9e9e2bed37cb7d295529397cad616a.pdf:pdf},
isbn = {9781538610343},
journal = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
pages = {367--374},
title = {{Semantic Segmentation of RGBD Videos with Recurrent Fully Convolutional Neural Networks}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w6/Yurdakul{\_}Semantic{\_}Segmentation{\_}of{\_}ICCV{\_}2017{\_}paper.pdf},
volume = {2018-Janua},
year = {2017}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/SvenG/Downloads/lstm.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@misc{Britz2015,
abstract = {This the third part of the Recurrent Neural Network Tutorial. In the previous part of the tutorial we implemented a RNN from scratch, but didn't go into detail on how Backpropagation Through Time (BPTT) algorithms calculates the gradients. In this part we'll give a brief overview of BPTT and explain how it differs from traditional backpropagation. We will then try to understand the vanishing gradient problem, which has led to the development of LSTMs and GRUs, two of the currently most popular and powerful models used in NLP (and other areas). The vanishing gradient problem was originally discovered by Sepp Hochreiter in 1991 and has been receiving attention again recently due to the increased application of deep architectures. To fully understand this part of the tutorial I recommend being familiar with how partial differentiation and basic backpropagation works. If you are not, you can find excellent tutorials here and here and here, in order of increasing difficulty.},
author = {Britz, Denny},
booktitle = {Wildml},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/8336da86aa6d99ca7ab7779f2d686cd4d8900dfc.html:html},
pages = {1--9},
title = {{Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients}},
url = {http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/},
urldate = {2020-05-30},
year = {2015}
}
@misc{Bazarevsky2018,
abstract = {Video segmentation is a widely used technique that enables movie directors and video content creators to separate the foreground of a scene from the background, and treat them as two different visual layers. By modifying or replacing the background, creators can convey a particular mood, transport themselves to a fun location or enhance the impact of the message. However, this operation has traditionally been performed as a time-consuming manual process (e.g. an artist rotoscoping every frame) or requires a studio environment with a green screen for real-time background removal (a technique referred to as chroma keying). In order to enable users to create this effect live in the viewfinder, we designed a new technique that is suitable for mobile phones.},
author = {Bazarevsky, Valentin and Tkachenka, Andrei},
booktitle = {Google AI Blog},
title = {{Mobile Real-time Video Segmentation}},
url = {https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html},
urldate = {2020-03-05},
year = {2018}
}
@article{Hoffmann2017,
abstract = {Deep Neural Networks (DNNs) are widely used for complex applications, such as image and voice processing. Two varieties of DNNs, namely Convolutional Neuronal Networks (CNNs) and Recurrent Neuronal Networks (RNNs), are particu- larly popular regarding recent success for industrial applications. While CNNs are typically used for computer vision applications like object recognition, RNNs are well suited for time variant problems due to their recursive structure. Even though CNNs and RNNs belong to the family of DNNs, their implementation shows substantial differences. Besides more common Central Processing Unit (CPU) and Graphic processing Unit (GPU) implementations, Field Programmable Gate Array (FPGA) implementations offer great potential. Recent evaluations have shown significant benefits of FPGA implementations of DNNs over CPUs and GPUs. In this paper, we compare current FPGA implementations of CNNs and RNNs and analyze their optimizations. With this, we provide insights regarding the specific benefits and drawbacks of recent FPGA implementations of DNNs.},
annote = {Rnn suited for time variant problems

hidden recurent state

issues learning long-term dependencies},
author = {Hoffmann, Javier and Navarro, Osvaldo and Florian, K and Jan{\ss}en, Benedikt and Michael, H},
file = {:C$\backslash$:/Users/SvenG/Downloads/pesaro{\_}2017{\_}3{\_}20{\_}68006.pdf:pdf},
isbn = {9781612085494},
journal = {Pesaro 2017},
keywords = {convolutional,deep learning,neural net-,recurrent},
number = {c},
pages = {33--39},
title = {{A Survey on CNN and RNN Implementations}},
year = {2017}
}
@article{Zhao2017,
abstract = {We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.},
archivePrefix = {arXiv},
arxivId = {1704.08545},
author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
doi = {10.1007/978-3-030-01219-9_25},
eprint = {1704.08545},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/35fe3dd3350c32467030884337dde10d5e20ff99.pdf:pdf},
isbn = {9783030012182},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {High-resolution,Real-time,Semantic segmentation},
month = {apr},
pages = {418--434},
title = {{ICNet for Real-Time Semantic Segmentation on High-Resolution Images}},
url = {https://hszhao.github.io/papers/eccv18{\_}icnet.pdf http://arxiv.org/abs/1704.08545},
volume = {11207 LNCS},
year = {2017}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170× faster than the recent leading method R-CNN (and 24-64× faster overall), while achieving better or comparable accuracy on Pascal VOC 2007. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f1eaf6c7f76a6a57d72f2b017be3eecc4b1ec68f.pdf:pdf},
isbn = {9783319105772},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {346--361},
title = {{Spatial pyramid pooling in deep convolutional networks for visual recognition}},
url = {https://arxiv.org/pdf/1406.4729.pdf},
volume = {8691 LNCS},
year = {2014}
}
@article{Hochreiter1998,
abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the decaying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
doi = {10.1142/S0218488598000094},
file = {:C$\backslash$:/Users/SvenG/Downloads/The{\_}Vanishing{\_}Gradient{\_}Problem{\_}During{\_}Learning{\_}Rec.pdf:pdf},
issn = {02184885},
journal = {International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems},
keywords = {Long Short-Term Memory,Long-term dependencies,Recurrent neural nets,Vanishing gradient},
number = {2},
pages = {107--116},
title = {{The vanishing gradient problem during learning recurrent neural nets and problem solutions}},
volume = {6},
year = {1998}
}
@article{Pfeuffer2_2019,
abstract = {Semantic Segmentation is an important module for autonomous robots such as self-driving cars. The advantage of video segmentation approaches compared to single image segmentation is that temporal image information is considered, and their performance increases due to this. Hence, single image segmentation approaches are extended by recurrent units such as convolutional LSTM (convLSTM) cells, which are placed at suitable positions in the basic network architecture. However, a major critique of video segmentation approaches based on recurrent neural networks is their large parameter count and their computational complexity, and so, their inference time of one video frame takes up to 66 percent longer than their basic version. Inspired by the success of the spatial and depthwise separable convolutional neural networks, we generalize these techniques for convLSTMs in this work, so that the number of parameters and the required FLOPs are reduced significantly. Experiments on different datasets show that the segmentation approaches using the proposed, modified convLSTM cells achieve similar or slightly worse accuracy, but are up to 15 percent faster on a GPU than the ones using the standard convLSTM cells. Furthermore, a new evaluation metric is introduced, which measures the amount of flickering pixels in the segmented video sequence.},
annote = {mFP -{\textgreater} mean flickering pixels

RNN aplications (Related work)

Spatial LSTM {\"{a}}hnlich gut wie original, sogar slightly better},
archivePrefix = {arXiv},
arxivId = {1907.06876},
author = {Pfeuffer, Andreas and Dietmayer, Klaus},
eprint = {1907.06876},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/e219654a41815cc694aaf0bddbfefd66aa645f00.pdf:pdf},
month = {jul},
title = {{Separable Convolutional LSTMs for Faster Video Segmentation}},
url = {https://arxiv.org/pdf/1907.06876.pdf http://arxiv.org/abs/1907.06876},
year = {2019}
}
@article{Bolya2019,
abstract = {We present a simple, fully-convolutional model for real-time ({\textgreater}30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.},
annote = {Real time.

- breaks up instance segmentation into two parallel tasks: (1) generating a dictionary of non-local prototype masks over the entire image, and (2) predicting a set of linear combination coefficients per instance. (Introduction)

- high quality and fast (no repooling)

- similiarity to what and where pathway (biologically inspired)

- ms coco dataset},
archivePrefix = {arXiv},
arxivId = {1912.06218},
author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
eprint = {1912.06218},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolya et al. - 2019 - YOLACT Better Real-time Instance Segmentation.pdf:pdf},
month = {dec},
number = {1},
pages = {1--12},
title = {{YOLACT++: Better Real-time Instance Segmentation}},
url = {http://arxiv.org/abs/1912.06218},
year = {2019}
}
