Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/fcd44ab4ebeeb504481350732f21701bf5269fb4.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://arxiv.org/pdf/1511.00561.pdf},
volume = {39},
year = {2017}
}
@article{Ramirez2019,
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
annote = {music genres are vague and unclear, very subjective (intro and 4)

Music genre classification (MGC) is a multi label classification problem --{\textgreater} problems of overlapping genres (2 ML)

GTZAN dataset beeing one of the most famouse one (4)

4.1 {\"{u}}ber featers die zur classification genutzt werden (MEL - erkl{\"{a}}rung und die n{\"{a}}he zum geh{\"{o}}r)

(5.1 GTZAN genaue erkl{\"{a}}rung mit hintergrund infos
--{\textgreater} Qualit{\"{a}}t hinterfragt)},
archivePrefix = {arXiv},
arxivId = {1911.12618},
author = {Ram{\'{i}}rez, Jaime and Flores, M. Julia},
doi = {10.1007/s10844-019-00582-9},
eprint = {1911.12618},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de25c85cb64dad1cfe855e9d60e99583bdb2c8f4.pdf:pdf},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification algorithms,Datasets,Feed-forward neural networks,Machine learning,Music,Music information retrieval},
title = {{Machine learning for music genre: multifaceted review and experimentation with audioset}},
url = {https://arxiv.org/pdf/1911.12618.pdf},
year = {2019}
}
@article{Dhanachandra2015,
abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
doi = {10.1016/j.procs.2015.06.090},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/image-segmentation-using-k-means-clustering-algorithm-and-subtractive-clustering-algorithm (1).pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Image segmentation,K -means clustering,Median filter,Partial contrast stretching,Subtractive clustering},
pages = {764--771},
publisher = {Elsevier Masson SAS},
title = {{Image Segmentation Using K-means Clustering Algorithm and Subtractive Clustering Algorithm}},
url = {http://dx.doi.org/10.1016/j.procs.2015.06.090},
volume = {54},
year = {2015}
}
@article{Harley2017,
abstract = {We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a 'segmentation-aware' variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can replace DenseCRF inference with a cascade of segmentation-aware filters, and in optical flow we obtain clearly sharper responses than the ones obtained with comparable networks that do not use segmentation. In both cases segmentation-aware convolution yields systematic improvements over strong baselines.},
annote = {Introduction:
"The low-resolution issue has received substantial attention: for instance methods have been proposed for replacing the subsampling layers with resolution-preserving alternatives such as atrous convolution [9, 58, 43], or restoring the lost resolution via upsampling stages [39, 34]"


local foreground-backgreound segmentation mask
--{\textgreater} to enhance sharpness

3 steps:
(i) learn segmentation cues, (ii) use the cues to create local foreground masks, and (iii) use the masks together with convolution, to create foreground-focused convolution

--{\textgreater} genauer lesen},
archivePrefix = {arXiv},
arxivId = {1708.04607},
author = {Harley, Adam W. and Derpanis, Konstantinos G. and Kokkinos, Iasonas},
doi = {10.1109/ICCV.2017.539},
eprint = {1708.04607},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f4b4bf63e1059645301d338ff792ba43a7361f98.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5048--5057},
title = {{Segmentation-Aware Convolutional Networks Using Local Attention Masks}},
url = {https://arxiv.org/pdf/1708.04607.pdf},
volume = {2017-Octob},
year = {2017}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/56142d781c2c1231ffbda59efb6f96fc7b5b5b52.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1520--1528},
title = {{Learning deconvolution network for semantic segmentation}},
url = {https://arxiv.org/pdf/1505.04366.pdf},
volume = {2015 International Conference on Computer Vision, ICCV 2015},
year = {2015}
}
@article{Feng2017,
abstract = {Deep learning has been demonstrated its effectiveness and efficiency in music genre classification. However, the existing achievements still have several shortcomings which impair the performance of this classification task. In this paper, we propose a hybrid architecture which consists of the paralleling CNN and Bi-RNN blocks. They focus on spatial features and temporal frame orders extraction respectively. Then the two outputs are fused into one powerful representation of musical signals and fed into softmax function for classification. The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.},
annote = {Cnn and RNN combinations (parrallel model)},
archivePrefix = {arXiv},
arxivId = {1712.08370},
author = {Feng, Lin and Liu, Shenlan and Yao, Jianing},
eprint = {1712.08370},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5f23aadfd868258a104d86eada9f6e856063135f.pdf:pdf},
number = {December},
pages = {1--13},
title = {{Music Genre Classification with Paralleling Recurrent Convolutional Neural Network}},
url = {http://arxiv.org/abs/1712.08370},
year = {2017}
}
@misc{Fei-Fei2017,
author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Johnson, Yeung - Unknown - Lecture 11 Detection and Segmentation.pdf:pdf},
pages = {17},
title = {{Lecture 11: Detection and Segmentation}},
url = {http://cs231n.stanford.edu/slides/2017/cs231n{\_}2017{\_}lecture11.pdf},
urldate = {2020-03-04},
year = {2017}
}
@article{Smith1979,
author = {Nobuyuki, Otsu},
doi = {10.1109/TSMC.1979.4310076},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/A{\_}Threshold{\_}Selection{\_}Method{\_}from{\_}gray-level{\_}Histograms.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {1},
pages = {62--66},
title = {{A Threshold Selection Method from Gray-Level Histograms}},
volume = {9},
year = {1979}
}
@article{AssessmentandTeachingof21stCenturySkills2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Assessment and Teaching of 21st Century Skills}},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SvenG/Downloads/azure.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{済無No Title No Title}},
volume = {53},
year = {2013}
}
@article{Pfeuffer2019,
abstract = {Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNN-based approach for up to 1.6 percent.},
annote = {vergleicht SegNet + LSTM mit ICNet + LSTM

-{\textgreater} verbesserte resultate, jedoch teilweise l{\"{a}}ngere comp. dauer},
archivePrefix = {arXiv},
arxivId = {1905.01058},
author = {Pfeuffer, Andreas and Schulz, Karina and Dietmayer, Klaus},
doi = {10.1109/IVS.2019.8813852},
eprint = {1905.01058},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfeuffer, Schulz, Dietmayer - 2019 - Semantic segmentation of video sequences with convolutional LSTMs.pdf:pdf},
isbn = {9781728105604},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {1441--1447},
title = {{Semantic segmentation of video sequences with convolutional LSTMs}},
url = {https://arxiv.org/pdf/1905.01058.pdf},
volume = {2019-June},
year = {2019}
}
@article{Bolya2019,
abstract = {We present a simple, fully-convolutional model for real-time ({\textgreater}30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.},
annote = {Real time.

- breaks up instance segmentation into two parallel tasks: (1) generating a dictionary of non-local prototype masks over the entire image, and (2) predicting a set of linear combination coefficients per instance. (Introduction)

- high quality and fast (no repooling)

- similiarity to what and where pathway (biologically inspired)

- ms coco dataset},
archivePrefix = {arXiv},
arxivId = {1912.06218},
author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
eprint = {1912.06218},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolya et al. - 2019 - YOLACT Better Real-time Instance Segmentation.pdf:pdf},
number = {1},
pages = {1--12},
title = {{YOLACT++: Better Real-time Instance Segmentation}},
url = {http://arxiv.org/abs/1912.06218},
year = {2019}
}
@article{Rieder2019,
annote = {used for image on page 10},
author = {Rieder, Mathias and Verbeet, Richard},
doi = {10.15480/882.2466},
file = {:C$\backslash$:/Users/SvenG/Downloads/Rieder{\_}Verbeet-Robot-Human-Learning{\_}for{\_}Robotic{\_}Picking{\_}Processes{\_}hicl{\_}2019.pdf:pdf},
keywords = {Computer Vision,Machine Learning,Object Detection,Picking Robots},
number = {October},
title = {{Robot-Human-Learning for Robotic Picking Processes}},
year = {2019}
}
@book{Szeliski2011,
author = {Szeliski, Richard},
booktitle = {Phylogenetic Networks},
doi = {10.1017/cbo9780511974076.010},
file = {:C$\backslash$:/Users/SvenG/Downloads/(Texts in Computer Science) Richard Szeliski - Computer Vision{\_} Algorithms and Applications-Springer (2011).pdf:pdf},
isbn = {9781848829343},
pages = {185--186},
publisher = {Springer},
title = {{Computer Vision: Algorithms and Applications}},
year = {2011}
}
@article{Han2019,
abstract = {Recent studies have greatly promoted the development of semantic segmentation. Most state-of-the-art methods adopt fully convolutional networks (FCNs) to accomplish this task, in which the fully connected layer is replaced with the convolution layer for dense prediction. However, standard convolution has limited ability in maintaining continuity between predicted labels as well as forcing local smooth. In this paper, we propose the dense convolution unit (DCU), which is more suitable for pixel-wise classification. The DCU adopts dense prediction instead of the center-prediction manner used in current convolution layers. The semantic label for every pixel is inferred from those overlapped center/off-center predictions from the perspective of probability. It helps to aggregate contexts and embeds connections between predictions, thus successfully generating accurate segmentation maps. DCU serves as the classification layer and is a better option than standard convolution in FCNs. This technique is applicable and beneficial to FCN-based state-of-the-art methods and works well in generating segmentation results. Ablation experiments on benchmark datasets validate the effectiveness and generalization ability of the proposed approach in semantic segmentation tasks.},
author = {Han, Chaoyi and Duan, Yiping and Tao, Xiaoming and Lu, Jianhua},
doi = {10.1109/ACCESS.2019.2908685},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/94f424c9bbaa0abb71874ddac144eab505539f48.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Dense convolution unit,fully convolutional network,overlapped prediction,semantic segmentation},
pages = {43369--43382},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf},
volume = {7},
year = {2014}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/75a1126f6710eeb85af855eb2b0d80946fcc6b6e.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1411.4038.pdf},
volume = {39},
year = {2017}
}
@article{Viswanathan2016,
abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
author = {Viswanathan, Ajay Prasadh},
doi = {10.18535/ijecs/v4i10.38},
file = {:C$\backslash$:/Users/SvenG/Downloads/21.pdf:pdf},
journal = {International Journal Of Engineering And Computer Science},
title = {{Music Genre Classification}},
year = {2016}
}
@article{Mittal2020,
annote = {"Image segmentation can be formulated as a classification problem of pixels with semantic labels (semantic segmentation) or partitioning of individual objects (instance segmentation)."

sehr nice introduction

explaines nicely the metrices used},
author = {Mittal, Mamta and Arora, Maanak and Pandey, Tushar and Goyal, Lalit Mohan},
doi = {10.1007/978-981-15-1100-4_3},
file = {:C$\backslash$:/Users/SvenG/Downloads/Segmentation{\_}Survey{\_}Arxiv.pdf:pdf},
keywords = {Deep Learning,Image Segmentation,Image segmentation,convolutional neural networks,deep learning,encoder-decoder models,generative models,instance segmentation,medical image segmentation.,recurrent models,semantic segmentation},
mendeley-tags = {Deep Learning,Image Segmentation},
number = {January},
pages = {41--63},
title = {{Image Segmentation Using Deep Learning : A Survey}},
year = {2020}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Liu2019,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Recently, there has been increasing interest in attempting convolutional neural networks (CNNs) to achieve the task. However, most of existing methods employ the mature CNN structures proposed in image recognition without any modification, which results in the learning features that are not adequate for music genre classification. Faced with the challenge of this issue, we fully exploit the low-level information from spectrograms of audios and develop a novel CNN architecture in this paper. The proposed CNN architecture takes the long contextual information into considerations, which transfers more suitable information for the decision-making layer. Various experiments on several benchmark datasets, including GTZAN, Ballroom, and Extended Ballroom, have verified the excellent performances of the proposed neural network. Codes and model will be available at "ttps://github.com/CaifengLiu/music-genre-classification".},
annote = {komplizierter approach aber gute resultate},
archivePrefix = {arXiv},
arxivId = {1901.08928},
author = {Liu, Caifeng and Feng, Lin and Liu, Guochao and Wang, Huibing and Liu, Shenglan},
eprint = {1901.08928},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ffcdc67c4ab342932bda320aa0217da88476e3de.pdf:pdf},
keywords = {all rights reserved,c 2019 elsevier ltd,cnn,music genre classification,spectrogram},
pages = {1--7},
title = {{Bottom-up Broadcast Neural Network For Music Genre Classification}},
url = {http://arxiv.org/abs/1901.08928},
year = {2019}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/87ba68678a1ae983cee474e4bfdd27257e45ca3d.pdf:pdf},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
url = {https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf},
volume = {36},
year = {1980}
}
@misc{Microsoft2019,
abstract = {This page covers how to use the depth camera in your Azure Kinect DK. The depth camera is the second of the two cameras. As covered in previous sections, the other camera is the RGB camera},
author = {Sych, Tetyana and Brent, Allen and Phil, Meadows and Microsoft},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c8bf20a2e5d841760fcee5c581f634d7052f749c.html:html},
title = {depth-camera @ docs.microsoft.com},
url = {https://docs.microsoft.com/de-de/azure/Kinect-dk/depth-camera},
urldate = {2020-03-04},
year = {2019}
}
@article{Chen2018,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed 'DeepLab' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0690ba31424310a90028533218d0afd25a829c8d.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1412.7062.pdf},
volume = {40},
year = {2018}
}
