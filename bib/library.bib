Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distin- guishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/rumelhart1986.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Flotation rate constant,Mutual information,Recovery,Support vector regression,Variable importance measurement},
month = {oct},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
url = {http://www.nature.com/articles/323533a0},
volume = {323},
year = {1986}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.

https://github.com/ds7711/music{\_}genre{\_}classification/blob/master/lda{\_}analysis.ipynb

confusion matrix code

This paper uses a "divide-and-conquer" approach to solve the classificationt ask.
1. split spectogram into 3 - second segements
2. make predicitons for each segment, combine predictions.
3 second approach based on previous papers. Input is mel-spectogram, which approxiamtes humans auditory system workings},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Choi2019,
abstract = {Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where side information about the labels is used to unveil the relationship between each other. In this work, we investigate the zero-shot learning in the music domain and organize two different setups of side information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations. Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning. Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in the music domain.},
annote = {zero-shot approach,

project onto a semantic space of labels

able to predict unseen labels (newly added) by using side information (musical ionstrument annotation vectors, word embeddings)},
archivePrefix = {arXiv},
arxivId = {1907.02670},
author = {Choi, Jeong and Lee, Jongpil and Park, Jiyoung and Nam, Juhan},
eprint = {1907.02670},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/539632b9c468a6629e956f80915d31601816a93a.pdf:pdf},
title = {{Zero-shot Learning for Audio-based Music Classification and Tagging}},
url = {http://arxiv.org/abs/1907.02670},
year = {2019}
}
@article{Tzanetakis2002,
abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a frame-work for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61{\%} for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
annote = {Classical approach (gaussian mixture model and k-nearest neighbor for a set of chosen extracted features)},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
file = {:C$\backslash$:/Users/SvenG/Downloads/download.pdf:pdf},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
month = {jul},
number = {5},
pages = {293--302},
title = {{Musical genre classification of audio signals}},
url = {http://ieeexplore.ieee.org/document/1021072/},
volume = {10},
year = {2002}
}
@article{Viswanathan2016,
abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
annote = {unser paper},
author = {Viswanathan, Ajay Prasadh},
doi = {10.18535/ijecs/v4i10.38},
file = {:C$\backslash$:/Users/SvenG/Downloads/21.pdf:pdf},
journal = {International Journal Of Engineering And Computer Science},
title = {{Music Genre Classification}},
year = {2016}
}
@article{Ramirez2019,
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
annote = {music genres are vague and unclear, very subjective (intro and 4)

Music genre classification (MGC) is a multi label classification problem --{\textgreater} problems of overlapping genres (2 ML)

GTZAN dataset beeing one of the most famouse one (4)

4.1 {\"{u}}ber featers die zur classification genutzt werden (MEL - erkl{\"{a}}rung und die n{\"{a}}he zum geh{\"{o}}r)

(5.1 GTZAN genaue erkl{\"{a}}rung mit hintergrund infos
--{\textgreater} Qualit{\"{a}}t hinterfragt)},
archivePrefix = {arXiv},
arxivId = {1911.12618},
author = {Ram{\'{i}}rez, Jaime and Flores, M. Julia},
doi = {10.1007/s10844-019-00582-9},
eprint = {1911.12618},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de25c85cb64dad1cfe855e9d60e99583bdb2c8f4.pdf:pdf},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification algorithms,Datasets,Feed-forward neural networks,Machine learning,Music,Music information retrieval},
title = {{Machine learning for music genre: multifaceted review and experimentation with audioset}},
url = {https://arxiv.org/pdf/1911.12618.pdf},
year = {2019}
}
@article{Feng2017,
abstract = {Deep learning has been demonstrated its effectiveness and efficiency in music genre classification. However, the existing achievements still have several shortcomings which impair the performance of this classification task. In this paper, we propose a hybrid architecture which consists of the paralleling CNN and Bi-RNN blocks. They focus on spatial features and temporal frame orders extraction respectively. Then the two outputs are fused into one powerful representation of musical signals and fed into softmax function for classification. The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.},
annote = {Cnn and RNN combinations (parrallel model)},
archivePrefix = {arXiv},
arxivId = {1712.08370},
author = {Feng, Lin and Liu, Shenlan and Yao, Jianing},
eprint = {1712.08370},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5f23aadfd868258a104d86eada9f6e856063135f.pdf:pdf},
number = {December},
pages = {1--13},
title = {{Music Genre Classification with Paralleling Recurrent Convolutional Neural Network}},
url = {http://arxiv.org/abs/1712.08370},
year = {2017}
}
@article{Dokania2019,
abstract = {Music genre is arguably one of the most important and discriminative information for music and audio content. Visual representation based approaches have been explored on spectrograms for music genre classification. However, lack of quality data and augmentation techniques makes it difficult to employ deep learning techniques successfully. We discuss the application of graph neural networks on such task due to their strong inductive bias, and show that combination of CNN and GNN is able to achieve state-of-the-art results on GTZAN, and AudioSet (Imbalanced Music) datasets. We also discuss the role of Siamese Neural Networks as an analogous to GNN for learning edge similarity weights. Furthermore, we also perform visual analysis to understand the field-of-view of our model into the spectrogram based on genre labels.},
annote = {GNNs and siamese network

used GTZAN dataset
--{\textgreater} "failes to capture recent trends in music"

70:30 train:test split},
archivePrefix = {arXiv},
arxivId = {1910.11117},
author = {Dokania, Shubham and Singh, Vasudev},
eprint = {1910.11117},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/25701c5d7e75b800245a9e103abd9fe6f7dda123.pdf:pdf},
month = {oct},
number = {2017},
title = {{Graph Representation learning for Audio {\&} Music genre Classification}},
url = {http://arxiv.org/abs/1910.11117},
year = {2019}
}
@article{Karatana2017,
abstract = {Categorizing music files according to their genre is a challenging task in the area of music information retrieval (MIR). In this study, we compare the performance of two classes of models. The first is a deep learning approach wherein a CNN model is trained end-to-end, to predict the genre label of an audio signal, solely using its spectrogram. The second approach utilizes hand-crafted features, both from the time domain and the frequency domain. We train four traditional machine learning classifiers with these features and compare their performance. The features that contribute the most towards this multi-class classification task are identified. The experiments are conducted on the Audio set data set and we report an AUC value of 0.894 for an ensemble classifier which combines the two proposed approaches.},
annote = {spectogram analysis vs hand crafted features

compares multiple ml classsifiers},
archivePrefix = {arXiv},
arxivId = {1804.01149},
author = {Karatana, Ali and Yildiz, Oktay},
doi = {10.1109/siu.2017.7960694},
eprint = {1804.01149},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/7915f9109576eeb26e33dad27b705efd509a1d17.pdf:pdf},
pages = {1--4},
title = {{Music genre classification with machine learning techniques}},
url = {https://arxiv.org/pdf/1804.01149.pdf},
year = {2017}
}
@article{Liu2019,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Recently, there has been increasing interest in attempting convolutional neural networks (CNNs) to achieve the task. However, most of existing methods employ the mature CNN structures proposed in image recognition without any modification, which results in the learning features that are not adequate for music genre classification. Faced with the challenge of this issue, we fully exploit the low-level information from spectrograms of audios and develop a novel CNN architecture in this paper. The proposed CNN architecture takes the long contextual information into considerations, which transfers more suitable information for the decision-making layer. Various experiments on several benchmark datasets, including GTZAN, Ballroom, and Extended Ballroom, have verified the excellent performances of the proposed neural network. Codes and model will be available at "ttps://github.com/CaifengLiu/music-genre-classification".},
annote = {komplizierter approach aber gute resultate
--{\textgreater} K{\"{o}}nnten wir versuchen


"Spectrograms of music data have been proved to be one effective tool to describe audio signals" (intro)

"it is neces- sary to design a specific CNN structure which can com- prehensively handle multi-scale of audio features."(intro)

"exploit an appropriate CNN model which can make full use of both the high-level semantic infor- mation and the low-level features from various music."(intro)

developed a "Bottom-up Broadcast Neural Network (BBNN), which adopts a relatively wide and shallow structure"


BM archi- tecture is that it maximally transmit and preserve all extracted feature maps to higher-layers so that the decision layers make a prediction based on all feature-maps in the network. Another practically useful aspect of BM design is that it aligns with the intuition that audio information should be perceived at various time-frequency scales simultaneously. (2.1)},
archivePrefix = {arXiv},
arxivId = {1901.08928},
author = {Liu, Caifeng and Feng, Lin and Liu, Guochao and Wang, Huibing and Liu, Shenglan},
eprint = {1901.08928},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ffcdc67c4ab342932bda320aa0217da88476e3de.pdf:pdf},
keywords = {all rights reserved,c 2019 elsevier ltd,cnn,music genre classification,spectrogram},
pages = {1--7},
title = {{Bottom-up Broadcast Neural Network For Music Genre Classification}},
url = {http://arxiv.org/abs/1901.08928},
year = {2019}
}
@article{Shelhamer2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/75a1126f6710eeb85af855eb2b0d80946fcc6b6e.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1411.4038.pdf http://arxiv.org/abs/1411.4038 http://ieeexplore.ieee.org/document/7478072/},
volume = {39},
year = {2014}
}
@book{Mallat1999,
author = {Mallat, St{\'{e}}phane},
edition = {2},
file = {:C$\backslash$:/Users/SvenG/Downloads/A{\_}Wavelet{\_}Tour{\_}of{\_}Signal{\_}Processing.pdf:pdf},
isbn = {0-12-466606-X},
publisher = {Academic Press},
title = {{A Wavelet Tour of Signal Processing}},
year = {1999}
}
@article{Jozefowicz2015,
abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/324fc9c732116fa81624faad07524039f193cede.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {2332--2340},
title = {{An empirical exploration of Recurrent Network architectures}},
url = {http://proceedings.mlr.press/v37/jozefowicz15.pdf},
volume = {3},
year = {2015}
}
@article{Cho2015,
abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {1409.1259},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
doi = {10.3115/v1/w14-4012},
eprint = {1409.1259},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/1eb09fecd75eb27825dce4f964b97f4f5cc399d7.pdf:pdf},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
url = {https://arxiv.org/pdf/1409.1259.pdf},
year = {2015}
}
@misc{COCO2016,
author = {{COCO Consortium}},
title = {{COCO - Common Objects in Context}},
url = {http://cocodataset.org/{\#}home http://mscoco.org/dataset/{\#}detections-leaderboard},
urldate = {2020-03-06},
year = {2016}
}
@article{Huang1997,
abstract = {In this paper we give a somewhat personal and perhaps biased overview of the field of Computer Vision. First, we define computer vision and give a very brief history of it. Then, we outline some of the reasons why computer vision is a very difficult research field. Finally, we discuss past, present, and future applications of computer vision. Especially, we give some examples of future applications which we think are very promising.},
author = {Huang, T S},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/fa9fbf250ec81d381e74386f697f399d083c38b8.pdf:pdf},
journal = {Report},
title = {{Computer Vision: Evolution and Promise}},
url = {https://cds.cern.ch/record/400313/files/p21.pdf},
year = {1997}
}
@article{Zhao2017,
abstract = {We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.},
archivePrefix = {arXiv},
arxivId = {1704.08545},
author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
doi = {10.1007/978-3-030-01219-9_25},
eprint = {1704.08545},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/35fe3dd3350c32467030884337dde10d5e20ff99.pdf:pdf},
isbn = {9783030012182},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {High-resolution,Real-time,Semantic segmentation},
month = {apr},
pages = {418--434},
title = {{ICNet for Real-Time Semantic Segmentation on High-Resolution Images}},
url = {https://hszhao.github.io/papers/eccv18{\_}icnet.pdf http://arxiv.org/abs/1704.08545},
volume = {11207 LNCS},
year = {2017}
}
@article{Hoffmann2017,
abstract = {Deep Neural Networks (DNNs) are widely used for complex applications, such as image and voice processing. Two varieties of DNNs, namely Convolutional Neuronal Networks (CNNs) and Recurrent Neuronal Networks (RNNs), are particu- larly popular regarding recent success for industrial applications. While CNNs are typically used for computer vision applications like object recognition, RNNs are well suited for time variant problems due to their recursive structure. Even though CNNs and RNNs belong to the family of DNNs, their implementation shows substantial differences. Besides more common Central Processing Unit (CPU) and Graphic processing Unit (GPU) implementations, Field Programmable Gate Array (FPGA) implementations offer great potential. Recent evaluations have shown significant benefits of FPGA implementations of DNNs over CPUs and GPUs. In this paper, we compare current FPGA implementations of CNNs and RNNs and analyze their optimizations. With this, we provide insights regarding the specific benefits and drawbacks of recent FPGA implementations of DNNs.},
annote = {Rnn suited for time variant problems

hidden recurent state

issues learning long-term dependencies},
author = {Hoffmann, Javier and Navarro, Osvaldo and Florian, K and Jan{\ss}en, Benedikt and Michael, H},
file = {:C$\backslash$:/Users/SvenG/Downloads/pesaro{\_}2017{\_}3{\_}20{\_}68006.pdf:pdf},
isbn = {9781612085494},
journal = {Pesaro 2017},
keywords = {convolutional,deep learning,neural net-,recurrent},
number = {c},
pages = {33--39},
title = {{A Survey on CNN and RNN Implementations}},
year = {2017}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/SvenG/Downloads/lstm.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@inproceedings{Csurka2013,
abstract = {In this work, we consider the evaluation of the semantic segmentation task. We discuss the strengths and limitations of the few existing measures, and propose new ways to evaluate semantic segmentation. First, we argue that a per-image score instead of one computed over the entire dataset brings a lot more insight. Second, we propose to take contours more carefully into account. Based on the conducted experiments, we suggest best practices for the evaluation. Finally, we present a user study we conducted to better understand how the quality of image segmentations is perceived by humans.},
author = {Csurka, Gabriela and Larlus, Diane and Perronnin, Florent},
booktitle = {Procedings of the British Machine Vision Conference 2013},
doi = {10.5244/C.27.32},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/02ce1ae116f9cd4bc4f9bb7ee7dda229eb556f84.pdf:pdf},
isbn = {1-901725-49-9},
pages = {32.1--32.11},
publisher = {British Machine Vision Association},
title = {{What is a good evaluation measure for semantic segmentation?}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf http://www.bmva.org/bmvc/2013/Papers/paper0032/index.html},
year = {2013}
}
@article{Hochreiter1998,
abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the decaying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
doi = {10.1142/S0218488598000094},
file = {:C$\backslash$:/Users/SvenG/Downloads/The{\_}Vanishing{\_}Gradient{\_}Problem{\_}During{\_}Learning{\_}Rec.pdf:pdf},
issn = {02184885},
journal = {International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems},
keywords = {Long Short-Term Memory,Long-term dependencies,Recurrent neural nets,Vanishing gradient},
number = {2},
pages = {107--116},
title = {{The vanishing gradient problem during learning recurrent neural nets and problem solutions}},
volume = {6},
year = {1998}
}
@article{Hernandez-Garcia2018,
abstract = {The impressive success of modern deep neural networks on computer vision tasks has been achieved through models of very large capacity compared to the number of available training examples. This overparameterization is often said to be controlled with the help of different regularization techniques, mainly weight decay and dropout. However, since these techniques reduce the effective capacity of the model, typically even deeper and wider architectures are required to compensate for the reduced capacity. Therefore, there seems to be a waste of capacity in this practice. In this paper we build upon recent research that suggests that explicit regularization may not be as important as widely believed and carry out an ablation study that concludes that weight decay and dropout may not be necessary for object recognition if enough data augmentation is introduced.},
archivePrefix = {arXiv},
arxivId = {1802.07042},
author = {Hern{\'{a}}ndez-Garc{\'{i}}a, Alex and K{\"{o}}nig, Peter},
eprint = {1802.07042},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/23c311d56190604f8aed18a36bdbdded702bc5bb.pdf:pdf},
pages = {1--5},
title = {{Do deep nets really need weight decay and dropout?}},
url = {http://arxiv.org/abs/1802.07042},
year = {2018}
}
@incollection{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/07045f87709d0b7b998794e9fa912c0aba912281.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
month = {may},
pages = {234--241},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {https://arxiv.org/pdf/1505.04597.pdf http://link.springer.com/10.1007/978-3-319-24574-4{\_}28},
volume = {9351},
year = {2015}
}
@article{Lillicrap2019,
abstract = {It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.},
author = {Lillicrap, Timothy P. and Santoro, Adam},
doi = {10.1016/j.conb.2019.01.011},
file = {:C$\backslash$:/Users/SvenG/Downloads/1-s2.0-S0959438818302009-main.pdf:pdf},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
month = {apr},
pages = {82--89},
pmid = {30851654},
publisher = {Elsevier Ltd},
title = {{Backpropagation through time and the brain}},
url = {https://doi.org/10.1016/j.conb.2019.01.011 https://linkinghub.elsevier.com/retrieve/pii/S0959438818302009},
volume = {55},
year = {2019}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao},
doi = {[]},
eprint = {1506.04214},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4a861d29f36d2e4f03477c5df2730c579d8394d3.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {2--3},
title = {{Convolutional LSTM Network}},
url = {http://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting},
year = {2015}
}
@misc{Britz2015,
abstract = {This the third part of the Recurrent Neural Network Tutorial. In the previous part of the tutorial we implemented a RNN from scratch, but didn't go into detail on how Backpropagation Through Time (BPTT) algorithms calculates the gradients. In this part we'll give a brief overview of BPTT and explain how it differs from traditional backpropagation. We will then try to understand the vanishing gradient problem, which has led to the development of LSTMs and GRUs, two of the currently most popular and powerful models used in NLP (and other areas). The vanishing gradient problem was originally discovered by Sepp Hochreiter in 1991 and has been receiving attention again recently due to the increased application of deep architectures. To fully understand this part of the tutorial I recommend being familiar with how partial differentiation and basic backpropagation works. If you are not, you can find excellent tutorials here and here and here, in order of increasing difficulty.},
author = {Britz, Denny},
booktitle = {Wildml},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/8336da86aa6d99ca7ab7779f2d686cd4d8900dfc.html:html},
pages = {1--9},
title = {{Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients}},
url = {http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/},
urldate = {2020-05-30},
year = {2015}
}
@article{Sharma2020,
abstract = {In this paper, we propose an end to end solution for image matting i.e high-precision extraction of foreground objects from natural images. Image matting and background detection can be achieved easily through chroma keying in a studio setting when the background is either pure green or blue. Nonetheless, image matting in natural scenes with complex and uneven depth backgrounds remains a tedious task that requires human intervention. To achieve complete automatic foreground extraction in natural scenes, we propose a method that assimilates semantic segmentation and deep image matting processes into a single network to generate detailed semantic mattes for image composition task. The contribution of our proposed method is two-fold, firstly it can be interpreted as a fully automated semantic image matting method and secondly as a refinement of existing semantic segmentation models. We propose a novel model architecture as a combination of segmentation and matting that unifies the function of upsampling and downsampling operators with the notion of attention. As shown in our work, attention guided downsampling and upsampling can extract high-quality boundary details, unlike other normal downsampling and upsampling techniques. For achieving the same, we utilized an attention guided encoder-decoder framework which does unsupervised learning for generating an attention map adaptively from the data to serve and direct the upsampling and downsampling operators. We also construct a fashion e-commerce focused dataset with high-quality alpha mattes to facilitate the training and evaluation for image matting.},
annote = {The major reason behind the fact that SegNet is better in recovering boundary details is that unpooling utilizes max- pooling's index guidance for upsampling. On the other hand, bi-linearly interpolated feature maps fail to emphasize on the boundary details. In order to record the boundary locations, the responses of the shallow layers of the network are used to project the excitation of various index locations on the feature maps into an attention mask},
archivePrefix = {arXiv},
arxivId = {2003.03613},
author = {Sharma, Rishab and Deora, Rahul and Vishvakarma, Anirudha},
eprint = {2003.03613},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/1c277b80530f47601aed44896db8adeda88f7fbe.pdf:pdf},
title = {{AlphaNet: An Attention Guided Deep Network for Automatic Image Matting}},
url = {http://arxiv.org/abs/2003.03613},
year = {2020}
}
@article{Hochreiter1991,
abstract = {Ich versichere, da{\ss} ich diese Diplomarbeit selbst{\"{a}}ndig verfa{\ss}t und keine anderen als die ange-geben Quellen und Hilfsmittel benutzt habe.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Hochreiter, Sepp},
eprint = {1511.07289},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f5f21cc907bc296e55ae87ce1319dfda644435b2.pdf:pdf},
issn = {18168957 18163459},
journal = {Master's thesis, Institut f{\"{u}}r Informatik, Technische Universit{\"{a}}t, Munchen},
pages = {1--71},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
url = {http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
year = {1991}
}
@misc{Fei-Fei2017,
author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Johnson, Yeung - Unknown - Lecture 11 Detection and Segmentation.pdf:pdf},
pages = {17},
title = {{Lecture 11: Detection and Segmentation}},
url = {http://cs231n.stanford.edu/slides/2017/cs231n{\_}2017{\_}lecture11.pdf},
urldate = {2020-03-04},
year = {2017}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/aeea02a93d674e0a044a8715e767f3a372582604.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {https://arxiv.org/pdf/1412.6980.pdf http://arxiv.org/abs/1412.6980},
year = {2014}
}
@inproceedings{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Francois},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4b53c6b0193935e710e15a0e1ec3f9f25502e450.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1800--1807},
publisher = {IEEE},
title = {{Xception: Deep Learning with Depthwise Separable Convolutions}},
url = {https://arxiv.org/pdf/1610.02357.pdf http://arxiv.org/abs/1610.02357 http://ieeexplore.ieee.org/document/8099678/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Smith2017,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate 'reasonable bounds' - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
annote = {allows to overcome saddle points where the gradient is small},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c95a65738554b82fbae159923ab834728500c7b3.pdf:pdf},
isbn = {9781509048229},
number = {April},
pages = {464--472},
title = {{Cyclical learning rates for training neural networks}},
url = {https://arxiv.org/pdf/1506.01186.pdf},
year = {2017}
}
@book{Suzuki2017,
author = {Suzuki, Kenji},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/40bd52ca059a531a7c0eb9742518f9624b5d8b5c.pdf:pdf},
isbn = {9789533072203},
number = {January 2013},
pages = {265},
title = {{Artificial Neural Networks - Methodological Advances and Biomedical Applications}},
url = {https://www.researchgate.net/profile/Kenji{\_}Suzuki4/publication/319316102{\_}Artificial{\_}Neural{\_}Networks{\_}-{\_}Methodological{\_}Advances{\_}and{\_}Biomedical{\_}Applications/links/59a42f16aca272a6461bb35e/Artificial-Neural-Networks-Methodological-Advances-and-Biomedical-Appl},
year = {2017}
}
@incollection{Kim2014,
abstract = {RGB-D sensors such as the Microsoft Kinect or the Asus Xtion are inexpensive 3D sensors. A depth image is computed by calculating the distortion of a known infrared light (IR) pattern which is projected into the scene. While these sensors are great devices they have some limitations. The distance they can measure is limited and they suffer from reflection problems on transparent, shiny, or very matte and absorbing objects. If more than one RGB-D camera is used the IR patterns interfere with each other. This results in a massive loss of depth information. In this paper, we present a simple and powerful method to overcome these problems. We propose a stereo RGB-D camera system which uses the pros of RGB-D cameras and combine them with the pros of stereo camera systems. The idea is to utilize the IR images of each two sensors as a stereo pair to generate a depth map. The IR patterns emitted by IR projectors are exploited here to enhance the dense stereo matching even if the observed objects or surfaces are texture-less or transparent. The resulting disparity map is then fused with the depth map offered by the RGB-D sensor to fill the regions and the holes that appear because of interference, or due to transparent or reflective objects. Our results show that the density of depth information is increased especially for transparent, shiny or matte objects.},
address = {Cham},
author = {Alhwarin, Faraj and Ferrein, Alexander and Scholl, Ingrid},
booktitle = {IEEE Transactions on Consumer Electronics},
doi = {10.1007/978-3-319-13560-1_33},
editor = {Pham, Duc-Nghia and Park, Seong-Bae},
file = {:C$\backslash$:/Users/SvenG/Downloads/ir{\_}stereo{\_}pricai2014.pdf:pdf},
issn = {0098-3063},
month = {aug},
number = {3},
pages = {409--421},
publisher = {IEEE},
title = {{IR Stereo Kinect: Improving Depth Images by Combining Structured Light with IR Stereo}},
url = {http://ieeexplore.ieee.org/document/6626256/ http://link.springer.com/10.1007/978-3-319-13560-1{\_}33},
volume = {59},
year = {2014}
}
@article{Nabavi2018,
abstract = {We consider the problem of predicting semantic segmentation of future frames in a video. Given several observed frames in a video, our goal is to predict the semantic segmentation map of future frames that are not yet observed. A reliable solution to this problem is useful in many applications that require real-time decision making, such as autonomous driving. We propose a novel model that uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal information of observed frames for future prediction. We also extend our model to use bidirectional ConvLSTM to capture temporal information in both directions. Our proposed approach outperforms other state-of-the-art methods on the benchmark dataset.},
annote = {bidirectional LSTM},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {shahabeddin Nabavi, Seyed and Rochan, Mrigank and Yang and Wang},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/651e844eaa55e9b91b77f84d76dd997cd9141554.pdf:pdf},
journal = {British Machine Vision Conference 2018, BMVC 2018},
month = {jul},
pages = {1--12},
title = {{Future Semantic Segmentation with Convolutional LSTM}},
url = {https://arxiv.org/pdf/1807.07946.pdf http://arxiv.org/abs/1807.07946},
year = {2018}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/2c03df8b48bf3fa39054345bafabfeff15bfd11d.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {https://arxiv.org/pdf/1512.03385.pdf},
volume = {2016-Decem},
year = {2016}
}
@article{Lafferty1999,
abstract = {We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. Disciplines Numerical Analysis and Scientific Computing Comments Postprint version. Abstract We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discrimi-native Markov models based on directed graph-ical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.},
author = {Lafferty, John and Mccallum, Andrew and Pereira, Fernando},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/7192a31b9e86650e6bc9e81e8f6b133c23665e2b.pdf:pdf},
number = {June},
pages = {282--289},
title = {{Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data Abstract}},
url = {http://repository.upenn.edu/cis{\_}papersPublisherURL:http://portal.acm.org/citation.cfm?id=655813PublisherURL:http://portal.acm.org/citation.cfm?id=655813ThisconferencepaperisavailableatScholarlyCommons:http://repository.upenn.edu/cis{\_}papers/159},
volume = {2001},
year = {1999}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/87ba68678a1ae983cee474e4bfdd27257e45ca3d.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
month = {apr},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
url = {https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf http://link.springer.com/10.1007/BF00344251},
volume = {36},
year = {1980}
}
@article{Smith2018,
abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
annote = {- over/underfitting tradeoff explained
- validation loss use?},
archivePrefix = {arXiv},
arxivId = {1803.09820},
author = {Smith, Leslie N.},
eprint = {1803.09820},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/925aceb1f1084c51260d2abdc5c9c6776022ba92.pdf:pdf},
month = {mar},
pages = {1--21},
title = {{A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay}},
url = {http://arxiv.org/abs/1803.09820},
year = {2018}
}
@article{Jiao2019,
abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people's life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1907.09408v2},
author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
doi = {10.1109/ACCESS.2019.2939201},
eprint = {arXiv:1907.09408v2},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/1f3a6a2cd504384bae261fe2bfbfadfdbffe7af9.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Classification,deep learning,localization,object detection,typical pipelines},
number = {3},
pages = {128837--128868},
title = {{A survey of deep learning-based object detection}},
url = {https://arxiv.org/pdf/1907.09408.pdf},
volume = {7},
year = {2019}
}
@misc{Olah2015,
author = {Olah, Christopher},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5192834415fb75e40f97944b7d64f8980b4c1ec5.html:html},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2020-05-28},
year = {2015}
}
@article{Chen2018,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/85c171cc3862c5d922632f94387db267ccbbe7cc.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
month = {jun},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.}},
url = {https://arxiv.org/pdf/1606.00915.pdf http://ieeexplore.ieee.org/document/7913730/ http://arxiv.org/abs/1606.00915 http://www.ncbi.nlm.nih.gov/pubmed/28463186},
volume = {40},
year = {2018}
}
@inproceedings{Sandler2018,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00474},
eprint = {1801.04381},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/761aa7289825cc2253b54346b5df667ecbd8b2db.pdf:pdf},
isbn = {978-1-5386-6420-9},
issn = {10636919},
month = {jun},
pages = {4510--4520},
publisher = {IEEE},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
url = {https://arxiv.org/pdf/1801.04381.pdf https://ieeexplore.ieee.org/document/8578572/},
year = {2018}
}
@incollection{Chen2018b,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89{\%} and 82.1{\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
archivePrefix = {arXiv},
arxivId = {1802.02611},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_49},
eprint = {1802.02611},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/503c16d9cb1560f13a7d6baedf8c9f889b22459d.pdf:pdf},
isbn = {9783030012335},
issn = {16113349},
keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
month = {feb},
pages = {833--851},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {https://arxiv.org/pdf/1802.02611.pdf http://link.springer.com/10.1007/978-3-030-01234-2{\_}49},
volume = {11211 LNCS},
year = {2018}
}
@article{Visin2015,
abstract = {We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.},
archivePrefix = {arXiv},
arxivId = {1511.07053},
author = {Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Cho, Kyunghyun and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron},
doi = {10.1109/CVPRW.2016.60},
eprint = {1511.07053},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/e2449e7a57f3f18ef58203c155ead421b5c72850.pdf:pdf},
isbn = {978-1-5090-1437-8},
issn = {21607516},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = {nov},
pages = {426--433},
publisher = {IEEE},
title = {{ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1511.07053.pdf http://ieeexplore.ieee.org/document/7789550/ http://arxiv.org/abs/1511.07053},
year = {2015}
}
@article{Pfeuffer2020,
abstract = {Computer vision tasks such as semantic segmentation perform very well in good weather conditions, but if the weather turns bad, they have problems to achieve this performance in these conditions. One possibility to obtain more robust and reliable results in adverse weather conditions is to use video-segmentation approaches instead of commonly used single-image segmentation methods. Video-segmentation approaches capture temporal information of the previous video-frames in addition to current image information, and hence, they are more robust against disturbances, especially if they occur in only a few frames of the video-sequence. However, video-segmentation approaches, which are often based on recurrent neural networks, cannot be applied in real-time applications anymore, since their recurrent structures in the network are computational expensive. For instance, the inference time of the LSTM-ICNet, in which recurrent units are placed at proper positions in the single-segmentation approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence, in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the network so that it becomes real-time capable again. Experiments on different datasets and various weather conditions show that the inference time can be decreased by about 23 percent by these modifications, while they achieve similar performance than the LSTM-ICNet and outperform the single-segmentation approach enormously in adverse weather conditions.},
archivePrefix = {arXiv},
arxivId = {2007.00290},
author = {Pfeuffer, Andreas and Dietmayer, Klaus},
eprint = {2007.00290},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/18c552146c02ffe8ec168fd76f241be295ec11e6.pdf:pdf},
pages = {1--6},
title = {{Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation}},
url = {http://arxiv.org/abs/2007.00290},
year = {2020}
}
@article{Pfeuffer2_2019,
abstract = {Semantic Segmentation is an important module for autonomous robots such as self-driving cars. The advantage of video segmentation approaches compared to single image segmentation is that temporal image information is considered, and their performance increases due to this. Hence, single image segmentation approaches are extended by recurrent units such as convolutional LSTM (convLSTM) cells, which are placed at suitable positions in the basic network architecture. However, a major critique of video segmentation approaches based on recurrent neural networks is their large parameter count and their computational complexity, and so, their inference time of one video frame takes up to 66 percent longer than their basic version. Inspired by the success of the spatial and depthwise separable convolutional neural networks, we generalize these techniques for convLSTMs in this work, so that the number of parameters and the required FLOPs are reduced significantly. Experiments on different datasets show that the segmentation approaches using the proposed, modified convLSTM cells achieve similar or slightly worse accuracy, but are up to 15 percent faster on a GPU than the ones using the standard convLSTM cells. Furthermore, a new evaluation metric is introduced, which measures the amount of flickering pixels in the segmented video sequence.},
annote = {mFP -{\textgreater} mean flickering pixels

RNN aplications (Related work)

Spatial LSTM {\"{a}}hnlich gut wie original, sogar slightly better},
archivePrefix = {arXiv},
arxivId = {1907.06876},
author = {Pfeuffer, Andreas and Dietmayer, Klaus},
eprint = {1907.06876},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/e219654a41815cc694aaf0bddbfefd66aa645f00.pdf:pdf},
month = {jul},
title = {{Separable Convolutional LSTMs for Faster Video Segmentation}},
url = {https://arxiv.org/pdf/1907.06876.pdf http://arxiv.org/abs/1907.06876},
year = {2019}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/56142d781c2c1231ffbda59efb6f96fc7b5b5b52.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {may},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1505.04366.pdf http://arxiv.org/abs/1505.04366},
volume = {2015 Inter},
year = {2015}
}
@article{Voulodimos2018,
abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
doi = {10.1155/2018/7068349},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ca011427853d34ce4ec9ccafde8a70c9eacc3e21.pdf:pdf},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
pages = {7068349},
pmid = {29487619},
title = {{Deep Learning for Computer Vision: A Brief Review.}},
url = {http://downloads.hindawi.com/journals/cin/2018/7068349.pdf https://www.hindawi.com/journals/cin/2018/7068349/ http://www.ncbi.nlm.nih.gov/pubmed/29487619 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5816885},
volume = {2018},
year = {2018}
}
@book{Forsyth2003,
author = {Forsyth, David A (University Of California At Berkeley) and Ponce, Jean (University Of Illinois)},
booktitle = {Education},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ab3e1ab27cf19b656d695f3e9d2ac647e597effd.pdf:pdf},
isbn = {0130851981},
pages = {673},
publisher = {Prentice Hall},
title = {{Computer Vision: A Modern Approach}},
url = {http://luthuli.cs.uiuc.edu/{~}daf/book/bookpages/pdf/front.pdf},
year = {2003}
}
@article{Sherstinsky2020,
abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
annote = {math equation of rnn},
archivePrefix = {arXiv},
arxivId = {1808.03314},
author = {Sherstinsky, Alex},
doi = {10.1016/j.physd.2019.132306},
eprint = {1808.03314},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/d8556d24d62e5b242f8d544c7b58b5973a3696fd.pdf:pdf},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
keywords = {Convolutional input context windows,External input gate,LSTM,RNN,RNN unfolding/unrolling},
month = {mar},
number = {March},
pages = {132306},
title = {{Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network}},
url = {https://arxiv.org/pdf/1808.03314.pdf https://linkinghub.elsevier.com/retrieve/pii/S0167278919305974},
volume = {404},
year = {2020}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170× faster than the recent leading method R-CNN (and 24-64× faster overall), while achieving better or comparable accuracy on Pascal VOC 2007. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f1eaf6c7f76a6a57d72f2b017be3eecc4b1ec68f.pdf:pdf},
isbn = {9783319105772},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {346--361},
title = {{Spatial pyramid pooling in deep convolutional networks for visual recognition}},
url = {https://arxiv.org/pdf/1406.4729.pdf},
volume = {8691 LNCS},
year = {2014}
}
@article{Smith1979,
author = {Nobuyuki, Otsu},
doi = {10.1109/TSMC.1979.4310076},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/A{\_}Threshold{\_}Selection{\_}Method{\_}from{\_}gray-level{\_}Histograms.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {1},
pages = {62--66},
title = {{A Threshold Selection Method from Gray-Level Histograms}},
volume = {9},
year = {1979}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0690ba31424310a90028533218d0afd25a829c8d.pdf:pdf},
journal = {ICLR 2015},
month = {jun},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1412.7062.pdf http://arxiv.org/abs/1606.00915},
year = {2016}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
annote = {GRU},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/afaa0382a4025407225ae014e1244a95f0a572d4.pdf:pdf},
isbn = {9781937284961},
journal = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
pages = {1724--1734},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://arxiv.org/pdf/1406.1078v3.pdf},
year = {2014}
}
@article{Cvpr2020,
abstract = {To satisfy the stringent requirements on computational resources in the field of real-time semantic segmentation, most approaches focus on the hand-crafted design of light-weight segmentation networks. Recently, Neural Architecture Search (NAS) has been used to search for the optimal building blocks of networks automatically, but the network depth, downsampling strategy, and feature aggregation way are still set in advance by trial and error. In this paper, we propose a joint search framework, called AutoRTNet, to automate the design of these strategies. Specifically, we propose hyper-cells to jointly decide the network depth and downsampling strategy, and an aggregation cell to achieve automatic multi-scale feature aggregation. Experimental results show that AutoRTNet achieves 73.9{\%} mIoU on the Cityscapes test set and 110.0 FPS on an NVIDIA TitanXP GPU card with 768x1536 input images.},
archivePrefix = {arXiv},
arxivId = {2003.14226},
author = {Sun, Peng and Wu, Jiaxiang and Li, Songyuan and Lin, Peiwen and Huang, Junzhou and Li, Xi},
eprint = {2003.14226},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/b86e9f4df22ff5a339b1597a25bc1cb52062dd13.pdf:pdf},
keywords = {neural,real-time semantic segmentation},
month = {mar},
pages = {1--15},
title = {{Real-Time Semantic Segmentation via Auto Depth, Downsampling Joint Decision and Feature Aggregation}},
year = {2020}
}
@book{Szeliski2011,
author = {Szeliski, Richard},
booktitle = {Phylogenetic Networks},
doi = {10.1017/cbo9780511974076.010},
file = {:C$\backslash$:/Users/SvenG/Downloads/(Texts in Computer Science) Richard Szeliski - Computer Vision{\_} Algorithms and Applications-Springer (2011).pdf:pdf},
isbn = {9781848829343},
pages = {185--186},
publisher = {Springer},
title = {{Computer Vision: Algorithms and Applications}},
year = {2011}
}
@misc{Microsoft2019,
abstract = {This page covers how to use the depth camera in your Azure Kinect DK. The depth camera is the second of the two cameras. As covered in previous sections, the other camera is the RGB camera},
author = {Sych, Tetyana and Brent, Allen and Phil, Meadows and Microsoft},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c8bf20a2e5d841760fcee5c581f634d7052f749c.html:html},
title = {depth-camera @ docs.microsoft.com},
url = {https://docs.microsoft.com/de-de/azure/Kinect-dk/depth-camera},
urldate = {2020-03-04},
year = {2019}
}
@article{Harley2017,
abstract = {We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a 'segmentation-aware' variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can replace DenseCRF inference with a cascade of segmentation-aware filters, and in optical flow we obtain clearly sharper responses than the ones obtained with comparable networks that do not use segmentation. In both cases segmentation-aware convolution yields systematic improvements over strong baselines.},
annote = {Introduction:
"The low-resolution issue has received substantial attention: for instance methods have been proposed for replacing the subsampling layers with resolution-preserving alternatives such as atrous convolution [9, 58, 43], or restoring the lost resolution via upsampling stages [39, 34]"


local foreground-backgreound segmentation mask
--{\textgreater} to enhance sharpness

3 steps:
(i) learn segmentation cues, (ii) use the cues to create local foreground masks, and (iii) use the masks together with convolution, to create foreground-focused convolution

--{\textgreater} genauer lesen},
archivePrefix = {arXiv},
arxivId = {1708.04607},
author = {Harley, Adam W. and Derpanis, Konstantinos G. and Kokkinos, Iasonas},
doi = {10.1109/ICCV.2017.539},
eprint = {1708.04607},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f4b4bf63e1059645301d338ff792ba43a7361f98.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5048--5057},
title = {{Segmentation-Aware Convolutional Networks Using Local Attention Masks}},
url = {https://arxiv.org/pdf/1708.04607.pdf},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{Pfeuffer2019,
abstract = {Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNNbased approach for up to 1.6 percent.},
annote = {vergleicht SegNet + LSTM mit ICNet + LSTM

-{\textgreater} verbesserte resultate, jedoch teilweise l{\"{a}}ngere comp. dauer},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {Pfeuffer, Andreas and Schulz, Karina and Dietmayer, Klaus},
doi = {10.1109/IVS.2019.8813852},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfeuffer, Schulz, Dietmayer - 2019 - Semantic segmentation of video sequences with convolutional LSTMs.pdf:pdf},
isbn = {9781728105604},
title = {{Semantic Segmentation of Video Sequences with Convolutional LSTMs}},
url = {https://arxiv.org/pdf/1905.01058.pdf},
year = {2019}
}
@article{Dey2017,
abstract = {The paper evaluates three variants of the Gated Recurrent Unit (GRU) in recurrent neural networks (RNN) by reducing parameters in the update and reset gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRU-RNN variant models perform as well as the original GRU RNN model while reducing the computational expense.},
archivePrefix = {arXiv},
arxivId = {1701.05923},
author = {Dey, Rahul and Salem, Fathi M.},
doi = {10.1109/MWSCAS.2017.8053243},
eprint = {1701.05923},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/dey2017.pdf:pdf},
isbn = {978-1-5090-6389-5},
issn = {15483746},
journal = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
month = {jan},
number = {2},
pages = {1597--1600},
publisher = {IEEE},
title = {{Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks}},
url = {https://ieeexplore.ieee.org/document/8053243/ http://arxiv.org/abs/1701.05923},
volume = {2017-Augus},
year = {2017}
}
@article{Siam2018,
abstract = {Semantic segmentation has recently witnessed major progress, but most of the previous work focused on improving single image segmentation. In this paper, we introduce a novel approach to implicitly utilize temporal data in videos for online segmentation. This design receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. This architecture is tested for both binary and semantic video segmentation tasks. Experiments are conducted on the recent benchmarks in SegTrack V2, Davis, Camvid, and Synthia. Using recurrent fully convolutional networks improved the baseline network performance in all of our experiments. Namely, 5{\%} and 3{\%} improvement of F-measure in SegTrack2 and Davis respectively, 5.7{\%} and 1.6{\%} improvement in mean IoU in Synthia and Camvid. Thus, RFCN networks can be seen as a method to improve any baseline segmentation network by embedding them into a recurrent module that utilizes temporal data.},
archivePrefix = {arXiv},
arxivId = {1611.05435},
author = {Siam, Mennatullah and Valipour, Sepehr and Jagersand, Martin and Ray, Nilanjan},
doi = {10.1109/ICIP.2017.8296851},
eprint = {1611.05435},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/b288a369c6f05443cb794048065b7a86139733d3.pdf:pdf},
isbn = {9781509021758},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Authors contributed equally,Recurrent Networks,Video Semantic Segmentation},
pages = {3090--3094},
title = {{Convolutional gated recurrent networks for video segmentation}},
url = {https://arxiv.org/pdf/1611.05435.pdf},
volume = {2017-Septe},
year = {2018}
}
@article{Kim2013,
abstract = {The commercial RGB-D camera produces color images and their depth maps from a scene in real time. However, the active camera creates mixed depth data near the border of different objects, occasionally losing depth information of shiny and dark surfaces in the scene. Furthermore, noise is added to the depth map. In this paper, a new method is presented to resolve such mixed, lost, and noisy pixel problems of the RGB-D camera. In particular, mixed pixel areas are detected using common distance transform (CDT) values of color and depth pixels, and merged them to lost pixel regions. The merged regions are filled with neighboring depth information based on an edge-stopping convergence function; distance transform values of color edge pixels are used to form this function. In addition, a CDT-based joint multilateral filter (CDT-JMF) is used to remove noisy pixels. Experimental results show that the proposed method gives better performance than conventional hole filling methods and image filters. {\textcopyright} 2013 IEEE.},
author = {Kim, Sung-yeol and Kim, Manbae and Ho, Yo-sung},
doi = {10.1109/TCE.2013.6626256},
file = {:C$\backslash$:/Users/SvenG/Downloads/oe-19-9-8019.pdf:pdf},
issn = {0098-3063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {RGB-D camera,depth image filter,distancetransform,mixed pixel},
month = {aug},
number = {3},
pages = {681--689},
title = {{Depth image filter for mixed and noisy pixel removal in RGB-D camera systems}},
url = {http://ieeexplore.ieee.org/document/6626256/},
volume = {59},
year = {2013}
}
@misc{fastai,
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0e635d9163cae33cda6f62a875eb3f9c61dbf8d7.html:html},
title = {index @ www.fast.ai},
url = {https://www.fast.ai/}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Minaee2020,
abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
archivePrefix = {arXiv},
arxivId = {2001.05566},
author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
eprint = {2001.05566},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/dc6b78923fdd6d8f767a8d1145fd2b6fc847e249.pdf:pdf},
pages = {1--23},
title = {{Image Segmentation Using Deep Learning: A Survey}},
url = {http://arxiv.org/abs/2001.05566},
year = {2020}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/b5db3a69206f320d003149113ab56c8c1327e837.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
url = {https://arxiv.org/pdf/1405.0312.pdf},
volume = {8693 LNCS},
year = {2014}
}
@article{Garcia-Garcia2018,
abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we formulate the semantic segmentation problem and define the terminology of this field as well as interesting background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and goals. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. We also devote a part of the paper to review common loss functions and error metrics for this problem. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Martinez-Gonzalez, Pablo and Garcia-Rodriguez, Jose},
doi = {10.1016/j.asoc.2018.05.018},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/A survey on Deep learning techniques for image and video semantic segmentation.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Deep learning,Scene labeling,Semantic segmentation},
pages = {41--65},
publisher = {Elsevier B.V.},
title = {{A survey on deep learning techniques for image and video semantic segmentation}},
url = {https://doi.org/10.1016/j.asoc.2018.05.018},
volume = {70},
year = {2018}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ac723ee6a5c524d4139ff2ef177dbf1f0889a983.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {https://arxiv.org/pdf/1311.2524.pdf},
year = {2014}
}
@article{Yurdakul2017,
abstract = {Semantic segmentation of videos using neural networks is currently a popular task, the work done in this field is however mostly on RGB videos. The main reason for this is the lack of large RGBD video datasets, annotated with ground truth information at the pixel level. In this work, we use a synthetic RGBD video dataset to investigate the contribution of depth and temporal information to the video segmentation task using convolutional and recurrent neural network architectures. Our experiments show the addition of depth information improves semantic segmentation results and exploiting temporal information results in higher quality output segmentations.},
annote = {combines depth + rgb + videos},
author = {Yurdakul, Ekrem Emre and Yemez, Yucel},
doi = {10.1109/ICCVW.2017.51},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c02dbf756b9e9e2bed37cb7d295529397cad616a.pdf:pdf},
isbn = {9781538610343},
journal = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
pages = {367--374},
title = {{Semantic Segmentation of RGBD Videos with Recurrent Fully Convolutional Neural Networks}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017{\_}workshops/papers/w6/Yurdakul{\_}Semantic{\_}Segmentation{\_}of{\_}ICCV{\_}2017{\_}paper.pdf},
volume = {2018-Janua},
year = {2017}
}
@article{Gastal2010,
abstract = {Image matting aims at extracting foreground elements from an image by means of color and opacity (alpha) estimation. While a lot of progress has been made in recent years on improving the accuracy of matting techniques, one common problem persisted: the low speed of matte computation. We present the first real-time matting technique for natural images and videos. Our technique is based on the observation that, for small neighborhoods, pixels tend to share similar attributes. Therefore, independently treating each pixel in the unknown regions of a trimap results in a lot of redundant work. We show how this computation can be significantly and safely reduced by means of a careful selection of pairs of background and foreground samples. Our technique achieves speedups of up to two orders of magnitude compared to previous ones, while producing high-quality alpha mattes. The quality of our results has been verified through an independent benchmark. The speed of our technique enables, for the first time, real-time alpha matting of videos, and has the potential to enable a new class of exciting applications. {\textcopyright} 2010 The Eurographics Association and Blackwell Publishing Ltd.},
author = {Gastal, Eduardo S.L. and Oliveira, Manuel M.},
doi = {10.1111/j.1467-8659.2009.01627.x},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/074e39a1c533993dcc829d9996c6518608d01e49.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {I.4.6 [Image Processing and Computer Vision]: Segm},
number = {2},
pages = {575--584},
title = {{Shared sampling for real-time alpha matting}},
url = {https://www.inf.ufrgs.br/{~}eslgastal/SharedMatting/Gastal{\_}Oliveira{\_}EG2010{\_}Shared{\_}Matting.pdf},
volume = {29},
year = {2010}
}
@incollection{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/20a78d3145279dcd799cd7a856ae2714f4863a16.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
keywords = {Convolutional neural network,Real-time object detection},
month = {dec},
pages = {21--37},
title = {{SSD: Single Shot MultiBox Detector}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0{\_}2},
volume = {9905 LNCS},
year = {2016}
}
@article{Rieder2019,
annote = {used for image on page 10},
author = {Rieder, Mathias and Verbeet, Richard},
doi = {10.15480/882.2466},
file = {:C$\backslash$:/Users/SvenG/Downloads/Rieder{\_}Verbeet-Robot-Human-Learning{\_}for{\_}Robotic{\_}Picking{\_}Processes{\_}hicl{\_}2019.pdf:pdf},
keywords = {Computer Vision,Machine Learning,Object Detection,Picking Robots},
number = {October},
title = {{Robot-Human-Learning for Robotic Picking Processes}},
year = {2019}
}
@inproceedings{Tokmakov2017,
abstract = {This paper addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a 'visual memory' in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given a video frame as input, our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the 'visual memory' specific to the video, acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results. For example, our approach outperforms the top method on the DAVIS dataset by nearly 6{\%}. We also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework.},
archivePrefix = {arXiv},
arxivId = {1704.05737},
author = {Tokmakov, Pavel and Alahari, Karteek and Schmid, Cordelia},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.480},
eprint = {1704.05737},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/82ddf559644446fb6b56ec228e55b2c4bbb61ab1.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
month = {oct},
pages = {4491--4500},
publisher = {IEEE},
title = {{Learning Video Object Segmentation with Visual Memory}},
url = {https://arxiv.org/pdf/1704.05737.pdf http://ieeexplore.ieee.org/document/8237742/},
volume = {2017-Octob},
year = {2017}
}
@article{Pascanu2013,
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. Copyright 2013 by the author(s).},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
eprint = {1211.5063},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/84069287da0a6b488b8c933f3cb5be759cb6237e.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
number = {PART 3},
pages = {2347--2355},
title = {{On the difficulty of training recurrent neural networks}},
url = {https://arxiv.org/pdf/1211.5063.pdf},
year = {2013}
}
@article{Ballas2016,
abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.},
archivePrefix = {arXiv},
arxivId = {1511.06432},
author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
eprint = {1511.06432},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/079edd5cf7968ac4759dfe72af2042cf6e990efc.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
month = {nov},
pages = {1--11},
title = {{Delving Deeper into Convolutional Networks for Learning Video Representations}},
url = {https://arxiv.org/pdf/1511.06432.pdf http://arxiv.org/abs/1511.06432},
year = {2015}
}
@book{Skansi2018,
address = {Cham},
author = {Skansi, Sandro},
doi = {10.1007/978-3-319-73004-2},
editor = {Mackie, Ian},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Dokumente/ML books/2018{\_}Book{\_}IntroductionToDeepLearning.pdf:pdf},
isbn = {978-3-319-73003-5},
pages = {187},
publisher = {Springer International Publishing},
series = {Undergraduate Topics in Computer Science},
title = {{Introduction to Deep Learning}},
url = {http://link.springer.com/10.1007/978-3-319-73004-2},
year = {2018}
}
@inproceedings{Valipour2017,
abstract = {Image segmentation is an important step in most visual tasks. While convolutional neural networks have shown to perform well on single image segmentation, to our knowledge, no study has been done on leveraging recurrent gated architectures for video segmentation. Accordingly, we propose and implement a novel method for online segmentation of video sequences that incorporates temporal data. The network is built from a fully convolutional network and a recurrent unit that works on a sliding window over the temporal data. We use convolutional gated recurrent unit that preserves the spatial information and reduces the parameters learned. Our method has the advantage that it can work in an online fashion instead of operating over the whole input batch of video frames. The network is tested on video segmentation benchmarks in Segtrack V2 and Davis. It proved to have 5{\%} improvement in Segtrack and 3{\%} improvement in Davis in F-measure over a plain fully convolutional network.},
annote = {LSTM and GRU outperform traditional recurrent architecutres (s2)

Nice RNN explanation
"Recurrent Neural Networks[25] are designed to incorporate sequential information into a neural network framework. " (s3)

"The gates control back propagation flow between each node" (s3)

- difference LSTM and GRU (s4)},
archivePrefix = {arXiv},
arxivId = {1606.00487},
author = {Valipour, Sepehr and Siam, Mennatullah and Jagersand, Martin and Ray, Nilanjan},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.11},
eprint = {1606.00487},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Valipour et al. - 2017 - Recurrent fully convolutional networks for video segmentation.pdf:pdf},
isbn = {9781509048229},
month = {mar},
pages = {29--36},
publisher = {IEEE},
title = {{Recurrent fully convolutional networks for video segmentation}},
url = {https://arxiv.org/pdf/1606.00487.pdf http://ieeexplore.ieee.org/document/7926594/},
year = {2017}
}
@inproceedings{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0012de6bec1f25599e4f02517637e531a71909b9.pdf:pdf},
isbn = {978-1-5090-5407-7},
keywords = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
month = {oct},
pages = {565--571},
publisher = {IEEE},
title = {{V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}},
url = {https://arxiv.org/pdf/1606.04797.pdf http://ieeexplore.ieee.org/document/7785132/},
year = {2016}
}
@article{Dhanachandra2015,
abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
doi = {10.1016/j.procs.2015.06.090},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/image-segmentation-using-k-means-clustering-algorithm-and-subtractive-clustering-algorithm (1).pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Image segmentation,K -means clustering,Median filter,Partial contrast stretching,Subtractive clustering},
pages = {764--771},
publisher = {Elsevier Masson SAS},
title = {{Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm}},
url = {http://dx.doi.org/10.1016/j.procs.2015.06.090 https://linkinghub.elsevier.com/retrieve/pii/S1877050915014143},
volume = {54},
year = {2015}
}
@misc{dataloader2020,
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/010526e01a891d134bfddf10546958220651dc4b.html:html},
title = {dataloader @ pytorch.org},
url = {https://pytorch.org/docs/stable/data.html{\#}torch.utils.data.DataLoader}
}
@article{AssessmentandTeachingof21stCenturySkills2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Assessment and Teaching of 21st Century Skills}},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SvenG/Downloads/azure.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Kinect Documentation}},
volume = {53},
year = {2013}
}
@article{Han2019,
abstract = {Recent studies have greatly promoted the development of semantic segmentation. Most state-of-the-art methods adopt fully convolutional networks (FCNs) to accomplish this task, in which the fully connected layer is replaced with the convolution layer for dense prediction. However, standard convolution has limited ability in maintaining continuity between predicted labels as well as forcing local smooth. In this paper, we propose the dense convolution unit (DCU), which is more suitable for pixel-wise classification. The DCU adopts dense prediction instead of the center-prediction manner used in current convolution layers. The semantic label for every pixel is inferred from those overlapped center/off-center predictions from the perspective of probability. It helps to aggregate contexts and embeds connections between predictions, thus successfully generating accurate segmentation maps. DCU serves as the classification layer and is a better option than standard convolution in FCNs. This technique is applicable and beneficial to FCN-based state-of-the-art methods and works well in generating segmentation results. Ablation experiments on benchmark datasets validate the effectiveness and generalization ability of the proposed approach in semantic segmentation tasks.},
author = {Han, Chaoyi and Duan, Yiping and Tao, Xiaoming and Lu, Jianhua},
doi = {10.1109/ACCESS.2019.2908685},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/94f424c9bbaa0abb71874ddac144eab505539f48.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Dense convolution unit,fully convolutional network,overlapped prediction,semantic segmentation},
pages = {43369--43382},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf},
volume = {7},
year = {2014}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381.pdf:pdf},
month = {dec},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/3b2697d76f035304bfeb57f6a682224c87645065.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {https://arxiv.org/pdf/1409.0575.pdf},
volume = {115},
year = {2015}
}
@article{Philipp2017,
abstract = {Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities "solve" the exploding gradient problem, we show that this is not the case in general and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients. ResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks. We show this is a direct consequence of the Pythagorean equation. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.},
archivePrefix = {arXiv},
arxivId = {1712.05577},
author = {Philipp, George and Song, Dawn and Carbonell, Jaime G.},
eprint = {1712.05577},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/55f673c7056fcfe001b3eee3d26b8867230d4476.pdf:pdf},
keywords = {deep learning,exploding gradients,neural networks,residual networks,van-},
number = {2014},
title = {{The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions}},
url = {http://arxiv.org/abs/1712.05577},
year = {2017}
}
@article{Bolya2019,
abstract = {We present a simple, fully-convolutional model for real-time ({\textgreater}30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.},
annote = {Real time.

- breaks up instance segmentation into two parallel tasks: (1) generating a dictionary of non-local prototype masks over the entire image, and (2) predicting a set of linear combination coefficients per instance. (Introduction)

- high quality and fast (no repooling)

- similiarity to what and where pathway (biologically inspired)

- ms coco dataset},
archivePrefix = {arXiv},
arxivId = {1912.06218},
author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
eprint = {1912.06218},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolya et al. - 2019 - YOLACT Better Real-time Instance Segmentation.pdf:pdf},
month = {dec},
number = {1},
pages = {1--12},
title = {{YOLACT++: Better Real-time Instance Segmentation}},
url = {http://arxiv.org/abs/1912.06218},
year = {2019}
}
@article{Lee2017,
abstract = {A Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) has driven tremendous improvements on an acoustic model based on Gaussian Mixture Model (GMM). However, these models based on a hybrid method require a forced aligned Hidden Markov Model (HMM) state sequence obtained from the GMM-based acoustic model. Therefore, it requires a long computation time for training both the GMM-based acoustic model and a deep learning-based acoustic model. In order to solve this problem, an acoustic model using CTC algorithm is proposed. CTC algorithm does not require the GMM-based acoustic model because it does not use the forced aligned HMM state sequence. However, previous works on a LSTM RNN-based acoustic model using CTC used a small-scale training corpus. In this paper, the LSTM RNN-based acoustic model using CTC is trained on a large-scale training corpus and its performance is evaluated. The implemented acoustic model has a performance of 6.18{\%} and 15.01{\%} in terms of Word Error Rate (WER) for clean speech and noisy speech, respectively. This is similar to a performance of the acoustic model based on the hybrid method.},
author = {Lee, Donghyun and Lim, Minkyu and Park, Hosung and Kang, Yoseb and Park, Jeong-Sik and Jang, Gil-Jin and Kim, Ji-Hwan},
doi = {10.1109/CC.2017.8068761},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/a4a00f5c2c8a24379c78ed2c1ec3c4d0f9c247a2.pdf:pdf},
issn = {1673-5447},
journal = {China Communications},
keywords = {acoustic model,connectionist temporal classification,large-scale training corpus,long short-term memory,recurrent neural network},
month = {sep},
number = {9},
pages = {23--31},
title = {{Long short-term memory recurrent neural network-based acoustic model using connectionist temporal classification on a large-scale training corpus}},
url = {https://static.googleusercontent.com/media/research.google.com/de//pubs/archive/43905.pdf http://ieeexplore.ieee.org/document/8068761/},
volume = {14},
year = {2017}
}
@misc{Bazarevsky2018,
abstract = {Video segmentation is a widely used technique that enables movie directors and video content creators to separate the foreground of a scene from the background, and treat them as two different visual layers. By modifying or replacing the background, creators can convey a particular mood, transport themselves to a fun location or enhance the impact of the message. However, this operation has traditionally been performed as a time-consuming manual process (e.g. an artist rotoscoping every frame) or requires a studio environment with a green screen for real-time background removal (a technique referred to as chroma keying). In order to enable users to create this effect live in the viewfinder, we designed a new technique that is suitable for mobile phones.},
author = {Bazarevsky, Valentin and Tkachenka, Andrei},
booktitle = {Google AI Blog},
title = {{Mobile Real-time Video Segmentation}},
url = {https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html},
urldate = {2020-03-05},
year = {2018}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4c17b93b0b486522da5a7c2a4de0e077253c20d2.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
month = {mar},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
volume = {42},
year = {2017}
}
@article{Cruz2012,
abstract = {Kinect is a device introduced in November 2010 as an accessory of Xbox 360. The acquired data has different and complementary natures, combining geometry with visual attributes. For this reason, Kinect is a flexible tool that can be used in applications from several areas such as: Computer Graphics, Image Processing, Computer Vision and Human-Machine Interaction. In this way, the Kinect is a widely used device in industry (games, robotics, theater performers, natural interfaces, etc.) and in research. We will initially present some concepts about the device: the architecture and the sensor. We then will discuss about the data acquisition process: capturing, representation and filtering. Capturing process consists of obtaining a colored image (RGB) and performing a depth measurement (D), with structured light technique. This data is represented by a structure called RGBD Image. We will also talk about the main tools available for developing applications on various platforms. Furthermore, we will discuss some recent projects based on RGBD Images. In particular, those related to Object Recognition, 3D Reconstruction, Augmented Reality, Image Processing, Robotic, and Interaction. In this survey, we will show some research developed by the academic community and some projects developed for the industry. We intend to show the basic principles to begin developing applications using Kinect, and present some projects developed at the VISGRAF Lab. And finally, we intend to discuss the new possibilities, challenges and trends raised by Kinect. {\textcopyright} 2012 IEEE.},
author = {Cruz, Leandro and Lucio, Djalma and Velho, Luiz},
doi = {10.1109/SIBGRAPI-T.2012.13},
file = {:C$\backslash$:/Users/SvenG/Downloads/KinectandRGBDImagesChallengesandApplication.pdf:pdf},
isbn = {9780769548302},
journal = {Proceedings: 25th SIBGRAPI - Conference on Graphics, Patterns and Images Tutorials, SIBGRAPI-T 2012},
keywords = {Kinect,RGBD Images},
number = {June 2014},
pages = {36--49},
title = {{Kinect and RGBD images: Challenges and applications}},
year = {2012}
}
@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/fcd44ab4ebeeb504481350732f21701bf5269fb4.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
month = {nov},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://arxiv.org/pdf/1511.00561.pdf http://arxiv.org/abs/1511.00561},
volume = {39},
year = {2015}
}
