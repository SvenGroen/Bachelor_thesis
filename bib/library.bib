Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Nabavi2018,
abstract = {We consider the problem of predicting semantic segmentation of future frames in a video. Given several observed frames in a video, our goal is to predict the semantic segmentation map of future frames that are not yet observed. A reliable solution to this problem is useful in many applications that require real-time decision making, such as autonomous driving. We propose a novel model that uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal information of observed frames for future prediction. We also extend our model to use bidirectional ConvLSTM to capture temporal information in both directions. Our proposed approach outperforms other state-of-the-art methods on the benchmark dataset.},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {shahabeddin Nabavi, Seyed and Rochan, Mrigank and Yang and Wang},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/651e844eaa55e9b91b77f84d76dd997cd9141554.pdf:pdf},
journal = {British Machine Vision Conference 2018, BMVC 2018},
month = {jul},
pages = {1--12},
title = {{Future Semantic Segmentation with Convolutional LSTM}},
url = {https://arxiv.org/pdf/1807.07946.pdf http://arxiv.org/abs/1807.07946},
year = {2018}
}
@inproceedings{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/78235aaf28039bbe83f2f10d6660ecb991c88eb9.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
month = {jun},
pages = {779--788},
publisher = {IEEE},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {https://arxiv.org/pdf/1506.02640v5.pdf http://ieeexplore.ieee.org/document/7780460/},
volume = {2016-Decem},
year = {2016}
}
@article{Bolya2019,
abstract = {We present a simple, fully-convolutional model for real-time ({\textgreater}30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.},
annote = {Real time.

- breaks up instance segmentation into two parallel tasks: (1) generating a dictionary of non-local prototype masks over the entire image, and (2) predicting a set of linear combination coefficients per instance. (Introduction)

- high quality and fast (no repooling)

- similiarity to what and where pathway (biologically inspired)

- ms coco dataset},
archivePrefix = {arXiv},
arxivId = {1912.06218},
author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
eprint = {1912.06218},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolya et al. - 2019 - YOLACT Better Real-time Instance Segmentation.pdf:pdf},
month = {dec},
number = {1},
pages = {1--12},
title = {{YOLACT++: Better Real-time Instance Segmentation}},
url = {http://arxiv.org/abs/1912.06218},
year = {2019}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ead027d47011fe5bd186c93b935728ae17ea3bdc.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
month = {dec},
pages = {1440--1448},
publisher = {IEEE},
title = {{Fast R-CNN}},
url = {https://arxiv.org/pdf/1504.08083.pdf http://arxiv.org/abs/1504.08083 http://ieeexplore.ieee.org/document/7410526/},
volume = {2015 Inter},
year = {2015}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao},
doi = {[]},
eprint = {1506.04214},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4a861d29f36d2e4f03477c5df2730c579d8394d3.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {2--3},
title = {{Convolutional LSTM Network}},
url = {http://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting},
year = {2015}
}
@misc{COCO2016,
author = {{COCO Consortium}},
title = {{COCO - Common Objects in Context}},
url = {http://cocodataset.org/{\#}home http://mscoco.org/dataset/{\#}detections-leaderboard},
urldate = {2020-03-06},
year = {2016}
}
@article{Zhao2017,
abstract = {We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.},
archivePrefix = {arXiv},
arxivId = {1704.08545},
author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
doi = {10.1007/978-3-030-01219-9_25},
eprint = {1704.08545},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/35fe3dd3350c32467030884337dde10d5e20ff99.pdf:pdf},
isbn = {9783030012182},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {High-resolution,Real-time,Semantic segmentation},
month = {apr},
pages = {418--434},
title = {{ICNet for Real-Time Semantic Segmentation on High-Resolution Images}},
url = {https://hszhao.github.io/papers/eccv18{\_}icnet.pdf http://arxiv.org/abs/1704.08545},
volume = {11207 LNCS},
year = {2017}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/4c17b93b0b486522da5a7c2a4de0e077253c20d2.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
month = {mar},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
volume = {42},
year = {2017}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/87ba68678a1ae983cee474e4bfdd27257e45ca3d.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
month = {apr},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
url = {https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf http://link.springer.com/10.1007/BF00344251},
volume = {36},
year = {1980}
}
@article{Rieder2019,
annote = {used for image on page 10},
author = {Rieder, Mathias and Verbeet, Richard},
doi = {10.15480/882.2466},
file = {:C$\backslash$:/Users/SvenG/Downloads/Rieder{\_}Verbeet-Robot-Human-Learning{\_}for{\_}Robotic{\_}Picking{\_}Processes{\_}hicl{\_}2019.pdf:pdf},
keywords = {Computer Vision,Machine Learning,Object Detection,Picking Robots},
number = {October},
title = {{Robot-Human-Learning for Robotic Picking Processes}},
year = {2019}
}
@article{Liu2019,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Recently, there has been increasing interest in attempting convolutional neural networks (CNNs) to achieve the task. However, most of existing methods employ the mature CNN structures proposed in image recognition without any modification, which results in the learning features that are not adequate for music genre classification. Faced with the challenge of this issue, we fully exploit the low-level information from spectrograms of audios and develop a novel CNN architecture in this paper. The proposed CNN architecture takes the long contextual information into considerations, which transfers more suitable information for the decision-making layer. Various experiments on several benchmark datasets, including GTZAN, Ballroom, and Extended Ballroom, have verified the excellent performances of the proposed neural network. Codes and model will be available at "ttps://github.com/CaifengLiu/music-genre-classification".},
annote = {komplizierter approach aber gute resultate},
archivePrefix = {arXiv},
arxivId = {1901.08928},
author = {Liu, Caifeng and Feng, Lin and Liu, Guochao and Wang, Huibing and Liu, Shenglan},
eprint = {1901.08928},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ffcdc67c4ab342932bda320aa0217da88476e3de.pdf:pdf},
keywords = {all rights reserved,c 2019 elsevier ltd,cnn,music genre classification,spectrogram},
pages = {1--7},
title = {{Bottom-up Broadcast Neural Network For Music Genre Classification}},
url = {http://arxiv.org/abs/1901.08928},
year = {2019}
}
@article{Ramirez2019,
abstract = {Music genre classification is one of the sub-disciplines of music information retrieval (MIR) with growing popularity among researchers, mainly due to the already open challenges. Although research has been prolific in terms of number of published works, the topic still suffers from a problem in its foundations: there is no clear and formal definition of what genre is. Music categorizations are vague and unclear, suffering from human subjectivity and lack of agreement. In its first part, this paper offers a survey trying to cover the many different aspects of the matter. Its main goal is give the reader an overview of the history and the current state-of-the-art, exploring techniques and datasets used to the date, as well as identifying current challenges, such as this ambiguity of genre definitions or the introduction of human-centric approaches. The paper pays special attention to new trends in machine learning applied to the music annotation problem. Finally, we also include a music genre classification experiment that compares different machine learning models using Audioset.},
annote = {music genres are vague and unclear, very subjective (intro and 4)

Music genre classification (MGC) is a multi label classification problem --{\textgreater} problems of overlapping genres (2 ML)

GTZAN dataset beeing one of the most famouse one (4)

4.1 {\"{u}}ber featers die zur classification genutzt werden (MEL - erkl{\"{a}}rung und die n{\"{a}}he zum geh{\"{o}}r)

(5.1 GTZAN genaue erkl{\"{a}}rung mit hintergrund infos
--{\textgreater} Qualit{\"{a}}t hinterfragt)},
archivePrefix = {arXiv},
arxivId = {1911.12618},
author = {Ram{\'{i}}rez, Jaime and Flores, M. Julia},
doi = {10.1007/s10844-019-00582-9},
eprint = {1911.12618},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de25c85cb64dad1cfe855e9d60e99583bdb2c8f4.pdf:pdf},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification algorithms,Datasets,Feed-forward neural networks,Machine learning,Music,Music information retrieval},
title = {{Machine learning for music genre: multifaceted review and experimentation with audioset}},
url = {https://arxiv.org/pdf/1911.12618.pdf},
year = {2019}
}
@article{Smith1979,
author = {Nobuyuki, Otsu},
doi = {10.1109/TSMC.1979.4310076},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/A{\_}Threshold{\_}Selection{\_}Method{\_}from{\_}gray-level{\_}Histograms.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {1},
pages = {62--66},
title = {{A Threshold Selection Method from Gray-Level Histograms}},
volume = {9},
year = {1979}
}
@inproceedings{Pfeuffer2019,
abstract = {We consider the problem of predicting semantic segmentation of future frames in a video. Given several observed frames in a video, our goal is to predict the semantic segmentation map of future frames that are not yet observed. A reliable solution to this problem is useful in many applications that require real-time decision making, such as autonomous driving. We propose a novel model that uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal information of observed frames for future prediction. We also extend our model to use bidirectional ConvLSTM to capture temporal information in both directions. Our proposed approach outperforms other state-of-the-art methods on the benchmark dataset.},
annote = {vergleicht SegNet + LSTM mit ICNet + LSTM

-{\textgreater} verbesserte resultate, jedoch teilweise l{\"{a}}ngere comp. dauer},
archivePrefix = {arXiv},
arxivId = {1807.07946},
author = {shahabeddin Nabavi, Seyed and Rochan, Mrigank and Wang, Yang},
booktitle = {British Machine Vision Conference 2018, BMVC 2018},
doi = {10.1109/IVS.2019.8813852},
eprint = {1807.07946},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfeuffer, Schulz, Dietmayer - 2019 - Semantic segmentation of video sequences with convolutional LSTMs.pdf:pdf},
isbn = {9781728105604},
pages = {1441--1447},
title = {{Future semantic segmentation with convolutional LSTM}},
url = {https://arxiv.org/pdf/1905.01058.pdf},
volume = {2019-June},
year = {2019}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ac723ee6a5c524d4139ff2ef177dbf1f0889a983.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {https://arxiv.org/pdf/1311.2524.pdf},
year = {2014}
}
@misc{Fei-Fei2017,
author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Johnson, Yeung - Unknown - Lecture 11 Detection and Segmentation.pdf:pdf},
pages = {17},
title = {{Lecture 11: Detection and Segmentation}},
url = {http://cs231n.stanford.edu/slides/2017/cs231n{\_}2017{\_}lecture11.pdf},
urldate = {2020-03-04},
year = {2017}
}
@article{Mittal2020,
annote = {"Image segmentation can be formulated as a classification problem of pixels with semantic labels (semantic segmentation) or partitioning of individual objects (instance segmentation)."

sehr nice introduction

explaines nicely the metrices used},
author = {Mittal, Mamta and Arora, Maanak and Pandey, Tushar and Goyal, Lalit Mohan},
doi = {10.1007/978-981-15-1100-4_3},
file = {:C$\backslash$:/Users/SvenG/Downloads/Segmentation{\_}Survey{\_}Arxiv.pdf:pdf},
keywords = {Deep Learning,Image Segmentation,Image segmentation,convolutional neural networks,deep learning,encoder-decoder models,generative models,instance segmentation,medical image segmentation.,recurrent models,semantic segmentation},
mendeley-tags = {Deep Learning,Image Segmentation},
number = {January},
pages = {41--63},
title = {{Image Segmentation Using Deep Learning : A Survey}},
year = {2020}
}
@article{Harley2017,
abstract = {We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a 'segmentation-aware' variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can replace DenseCRF inference with a cascade of segmentation-aware filters, and in optical flow we obtain clearly sharper responses than the ones obtained with comparable networks that do not use segmentation. In both cases segmentation-aware convolution yields systematic improvements over strong baselines.},
annote = {Introduction:
"The low-resolution issue has received substantial attention: for instance methods have been proposed for replacing the subsampling layers with resolution-preserving alternatives such as atrous convolution [9, 58, 43], or restoring the lost resolution via upsampling stages [39, 34]"


local foreground-backgreound segmentation mask
--{\textgreater} to enhance sharpness

3 steps:
(i) learn segmentation cues, (ii) use the cues to create local foreground masks, and (iii) use the masks together with convolution, to create foreground-focused convolution

--{\textgreater} genauer lesen},
archivePrefix = {arXiv},
arxivId = {1708.04607},
author = {Harley, Adam W. and Derpanis, Konstantinos G. and Kokkinos, Iasonas},
doi = {10.1109/ICCV.2017.539},
eprint = {1708.04607},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/f4b4bf63e1059645301d338ff792ba43a7361f98.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5048--5057},
title = {{Segmentation-Aware Convolutional Networks Using Local Attention Masks}},
url = {https://arxiv.org/pdf/1708.04607.pdf},
volume = {2017-Octob},
year = {2017}
}
@misc{Bazarevsky2018,
abstract = {Video segmentation is a widely used technique that enables movie directors and video content creators to separate the foreground of a scene from the background, and treat them as two different visual layers. By modifying or replacing the background, creators can convey a particular mood, transport themselves to a fun location or enhance the impact of the message. However, this operation has traditionally been performed as a time-consuming manual process (e.g. an artist rotoscoping every frame) or requires a studio environment with a green screen for real-time background removal (a technique referred to as chroma keying). In order to enable users to create this effect live in the viewfinder, we designed a new technique that is suitable for mobile phones.},
author = {Bazarevsky, Valentin and Tkachenka, Andrei},
booktitle = {Google AI Blog},
title = {{Mobile Real-time Video Segmentation}},
url = {https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html},
urldate = {2020-03-05},
year = {2018}
}
@article{Chen2018a,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed 'DeepLab' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/85c171cc3862c5d922632f94387db267ccbbe7cc.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
month = {apr},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1606.00915.pdf http://ieeexplore.ieee.org/document/7913730/},
volume = {40},
year = {2018}
}
@article{AssessmentandTeachingof21stCenturySkills2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Assessment and Teaching of 21st Century Skills}},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/SvenG/Downloads/azure.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{済無No Title No Title}},
volume = {53},
year = {2013}
}
@article{Dhanachandra2015,
abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
doi = {10.1016/j.procs.2015.06.090},
file = {:C$\backslash$:/Users/SvenG/OneDrive/Uni/Bachelor{\_}thesis/papers/image-segmentation-using-k-means-clustering-algorithm-and-subtractive-clustering-algorithm (1).pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Image segmentation,K -means clustering,Median filter,Partial contrast stretching,Subtractive clustering},
pages = {764--771},
publisher = {Elsevier Masson SAS},
title = {{Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm}},
url = {http://dx.doi.org/10.1016/j.procs.2015.06.090 https://linkinghub.elsevier.com/retrieve/pii/S1877050915014143},
volume = {54},
year = {2015}
}
@article{Han2019,
abstract = {Recent studies have greatly promoted the development of semantic segmentation. Most state-of-the-art methods adopt fully convolutional networks (FCNs) to accomplish this task, in which the fully connected layer is replaced with the convolution layer for dense prediction. However, standard convolution has limited ability in maintaining continuity between predicted labels as well as forcing local smooth. In this paper, we propose the dense convolution unit (DCU), which is more suitable for pixel-wise classification. The DCU adopts dense prediction instead of the center-prediction manner used in current convolution layers. The semantic label for every pixel is inferred from those overlapped center/off-center predictions from the perspective of probability. It helps to aggregate contexts and embeds connections between predictions, thus successfully generating accurate segmentation maps. DCU serves as the classification layer and is a better option than standard convolution in FCNs. This technique is applicable and beneficial to FCN-based state-of-the-art methods and works well in generating segmentation results. Ablation experiments on benchmark datasets validate the effectiveness and generalization ability of the proposed approach in semantic segmentation tasks.},
author = {Han, Chaoyi and Duan, Yiping and Tao, Xiaoming and Lu, Jianhua},
doi = {10.1109/ACCESS.2019.2908685},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/94f424c9bbaa0abb71874ddac144eab505539f48.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Dense convolution unit,fully convolutional network,overlapped prediction,semantic segmentation},
pages = {43369--43382},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://people.eecs.berkeley.edu/{~}jonlong/long{\_}shelhamer{\_}fcn.pdf},
volume = {7},
year = {2014}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/75a1126f6710eeb85af855eb2b0d80946fcc6b6e.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1411.4038.pdf http://arxiv.org/abs/1411.4038},
volume = {39},
year = {2014}
}
@book{Szeliski2011,
author = {Szeliski, Richard},
booktitle = {Phylogenetic Networks},
doi = {10.1017/cbo9780511974076.010},
file = {:C$\backslash$:/Users/SvenG/Downloads/(Texts in Computer Science) Richard Szeliski - Computer Vision{\_} Algorithms and Applications-Springer (2011).pdf:pdf},
isbn = {9781848829343},
pages = {185--186},
publisher = {Springer},
title = {{Computer Vision: Algorithms and Applications}},
year = {2011}
}
@article{Dong2019,
abstract = {Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61{\%} accuracy has been achieved in the 10-genre classification. However, it's still below the 70{\%} accuracy that humans could achieve in the same task. Here, we propose a new method that combines knowledge of human perception study in music genre classification and the neurophysiology of the auditory system. The method works by training a simple convolutional neural network (CNN) to classify a short segment of the music signal. Then, the genre of a music is determined by splitting it into short segments and then combining CNN's predictions from all short segments. After training, this method achieves human-level (70{\%}) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.},
annote = {compares cnn to spectrotemporal receptive fields.

very similar structure and results to original paper.},
archivePrefix = {arXiv},
arxivId = {1802.09697},
author = {Dong, Mingwen},
doi = {10.32470/ccn.2018.1153-0},
eprint = {1802.09697},
file = {:C$\backslash$:/Users/SvenG/Downloads/1802.09697.pdf:pdf},
pages = {1--6},
title = {{Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification}},
year = {2019}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/56142d781c2c1231ffbda59efb6f96fc7b5b5b52.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {may},
pages = {1520--1528},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {https://arxiv.org/pdf/1505.04366.pdf http://arxiv.org/abs/1505.04366},
volume = {2015 Inter},
year = {2015}
}
@article{Chen2018,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/0690ba31424310a90028533218d0afd25a829c8d.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
month = {jun},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {https://arxiv.org/pdf/1412.7062.pdf http://ieeexplore.ieee.org/document/7913730/ http://arxiv.org/abs/1606.00915},
volume = {40},
year = {2016}
}
@article{Viswanathan2016,
abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
author = {Viswanathan, Ajay Prasadh},
doi = {10.18535/ijecs/v4i10.38},
file = {:C$\backslash$:/Users/SvenG/Downloads/21.pdf:pdf},
journal = {International Journal Of Engineering And Computer Science},
title = {{Music Genre Classification}},
year = {2016}
}
@article{Gastal2010,
abstract = {Image matting aims at extracting foreground elements from an image by means of color and opacity (alpha) estimation. While a lot of progress has been made in recent years on improving the accuracy of matting techniques, one common problem persisted: the low speed of matte computation. We present the first real-time matting technique for natural images and videos. Our technique is based on the observation that, for small neighborhoods, pixels tend to share similar attributes. Therefore, independently treating each pixel in the unknown regions of a trimap results in a lot of redundant work. We show how this computation can be significantly and safely reduced by means of a careful selection of pairs of background and foreground samples. Our technique achieves speedups of up to two orders of magnitude compared to previous ones, while producing high-quality alpha mattes. The quality of our results has been verified through an independent benchmark. The speed of our technique enables, for the first time, real-time alpha matting of videos, and has the potential to enable a new class of exciting applications. {\textcopyright} 2010 The Eurographics Association and Blackwell Publishing Ltd.},
author = {Gastal, Eduardo S.L. and Oliveira, Manuel M.},
doi = {10.1111/j.1467-8659.2009.01627.x},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/074e39a1c533993dcc829d9996c6518608d01e49.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {I.4.6 [Image Processing and Computer Vision]: Segm},
number = {2},
pages = {575--584},
title = {{Shared sampling for real-time alpha matting}},
url = {https://www.inf.ufrgs.br/{~}eslgastal/SharedMatting/Gastal{\_}Oliveira{\_}EG2010{\_}Shared{\_}Matting.pdf},
volume = {29},
year = {2010}
}
@incollection{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/20a78d3145279dcd799cd7a856ae2714f4863a16.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
keywords = {Convolutional neural network,Real-time object detection},
month = {dec},
pages = {21--37},
title = {{SSD: Single Shot MultiBox Detector}},
url = {http://link.springer.com/10.1007/978-3-319-46448-0{\_}2},
volume = {9905 LNCS},
year = {2016}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/61612534716052a205c0747e99f34d977003bc88.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
month = {jun},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {39},
year = {2015}
}
@incollection{Chen2018b,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89{\%} and 82.1{\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
archivePrefix = {arXiv},
arxivId = {1802.02611},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_49},
eprint = {1802.02611},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/503c16d9cb1560f13a7d6baedf8c9f889b22459d.pdf:pdf},
isbn = {9783030012335},
issn = {16113349},
keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
month = {feb},
pages = {833--851},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {https://arxiv.org/pdf/1802.02611.pdf http://link.springer.com/10.1007/978-3-030-01234-2{\_}49},
volume = {11211 LNCS},
year = {2018}
}
@misc{Microsoft2019,
abstract = {This page covers how to use the depth camera in your Azure Kinect DK. The depth camera is the second of the two cameras. As covered in previous sections, the other camera is the RGB camera},
author = {Sych, Tetyana and Brent, Allen and Phil, Meadows and Microsoft},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/c8bf20a2e5d841760fcee5c581f634d7052f749c.html:html},
title = {depth-camera @ docs.microsoft.com},
url = {https://docs.microsoft.com/de-de/azure/Kinect-dk/depth-camera},
urldate = {2020-03-04},
year = {2019}
}
@article{Feng2017,
abstract = {Deep learning has been demonstrated its effectiveness and efficiency in music genre classification. However, the existing achievements still have several shortcomings which impair the performance of this classification task. In this paper, we propose a hybrid architecture which consists of the paralleling CNN and Bi-RNN blocks. They focus on spatial features and temporal frame orders extraction respectively. Then the two outputs are fused into one powerful representation of musical signals and fed into softmax function for classification. The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.},
annote = {Cnn and RNN combinations (parrallel model)},
archivePrefix = {arXiv},
arxivId = {1712.08370},
author = {Feng, Lin and Liu, Shenlan and Yao, Jianing},
eprint = {1712.08370},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/5f23aadfd868258a104d86eada9f6e856063135f.pdf:pdf},
number = {December},
pages = {1--13},
title = {{Music Genre Classification with Paralleling Recurrent Convolutional Neural Network}},
url = {http://arxiv.org/abs/1712.08370},
year = {2017}
}
@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:C$\backslash$:/Users/SvenG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/fcd44ab4ebeeb504481350732f21701bf5269fb4.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
month = {nov},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {https://arxiv.org/pdf/1511.00561.pdf http://arxiv.org/abs/1511.00561},
volume = {39},
year = {2015}
}
